{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/STS/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /data/ephemeral/home/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /data/ephemeral/home/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /data/ephemeral/home/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wandb\n",
    "import yaml\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from utils.tokenizer import get_tokenizer\n",
    "from data_loader.data_loaders import TextDataLoader\n",
    "from utils.util import set_seed\n",
    "from model.model import STSModel\n",
    "from utils.util import WandbCheckpointCallback\n",
    "from utils.clean import clean_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_1</th>\n",
       "      <th>sentence_2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>스릴도있고 반전도 있고 여느 한국영화 쓰레기들하고는 차원이 다르네요~</td>\n",
       "      <td>반전도 있고,사랑도 있고재미도있네요.</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>앗 제가 접근권한이 없다고 뜹니다;;</td>\n",
       "      <td>오, 액세스 권한이 없다고 합니다.</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>주택청약조건 변경해주세요.</td>\n",
       "      <td>주택청약 무주택기준 변경해주세요.</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>입사후 처음 대면으로 만나 반가웠습니다.</td>\n",
       "      <td>화상으로만 보다가 리얼로 만나니 정말 반가웠습니다.</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>뿌듯뿌듯 하네요!!</td>\n",
       "      <td>꼬옥 실제로 한번 뵈어요 뿌뿌뿌~!~!</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               sentence_1                    sentence_2  label\n",
       "0  스릴도있고 반전도 있고 여느 한국영화 쓰레기들하고는 차원이 다르네요~          반전도 있고,사랑도 있고재미도있네요.    2.2\n",
       "1                    앗 제가 접근권한이 없다고 뜹니다;;           오, 액세스 권한이 없다고 합니다.    4.2\n",
       "2                          주택청약조건 변경해주세요.            주택청약 무주택기준 변경해주세요.    2.4\n",
       "3                  입사후 처음 대면으로 만나 반가웠습니다.  화상으로만 보다가 리얼로 만나니 정말 반가웠습니다.    3.0\n",
       "4                              뿌듯뿌듯 하네요!!         꼬옥 실제로 한번 뵈어요 뿌뿌뿌~!~!    0.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "dev = pd.read_csv('data/dev.csv')\n",
    "test = pd.read_csv('data/test.csv')\n",
    "\n",
    "train.drop(columns=['id', 'source', 'binary-label'], inplace = True)\n",
    "dev.drop(columns=['id', 'source', 'binary-label'], inplace = True)\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9324 entries, 0 to 9323\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   sentence_1  9324 non-null   object \n",
      " 1   sentence_2  9324 non-null   object \n",
      " 2   label       9324 non-null   float64\n",
      "dtypes: float64(1), object(2)\n",
      "memory usage: 218.7+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    9324.000000\n",
       "mean       23.258151\n",
       "std        14.395189\n",
       "min         9.000000\n",
       "25%        14.000000\n",
       "50%        19.000000\n",
       "75%        28.000000\n",
       "max        98.000000\n",
       "Name: sentence_1, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['sentence_1'].apply(lambda x: len(x)).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_1</th>\n",
       "      <th>sentence_2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>스릴도있고 반전도 있고 여느 한국영화 쓰레기들하고는 차원이 다르네요</td>\n",
       "      <td>반전도 있고사랑도 있고재미도있네요</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>앗 제가 접근권한이 없다고 뜹니다</td>\n",
       "      <td>오 액세스 권한이 없다고 합니다</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>주택청약조건 변경해주세요</td>\n",
       "      <td>주택청약 무주택기준 변경해주세요</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>입사후 처음 대면으로 만나 반가웠습니다</td>\n",
       "      <td>화상으로만 보다가 리얼로 만나니 정말 반가웠습니다</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>뿌듯뿌듯 하네요</td>\n",
       "      <td>꼬옥 실제로 한번 뵈어요 뿌뿌뿌</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              sentence_1                   sentence_2  label\n",
       "0  스릴도있고 반전도 있고 여느 한국영화 쓰레기들하고는 차원이 다르네요           반전도 있고사랑도 있고재미도있네요    2.2\n",
       "1                     앗 제가 접근권한이 없다고 뜹니다            오 액세스 권한이 없다고 합니다    4.2\n",
       "2                          주택청약조건 변경해주세요            주택청약 무주택기준 변경해주세요    2.4\n",
       "3                  입사후 처음 대면으로 만나 반가웠습니다  화상으로만 보다가 리얼로 만나니 정말 반가웠습니다    3.0\n",
       "4                               뿌듯뿌듯 하네요            꼬옥 실제로 한번 뵈어요 뿌뿌뿌    0.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[['label']] = train[['label']].astype('float32')\n",
    "train['sentence_1'] = clean_texts(train['sentence_1'])\n",
    "train['sentence_2'] = clean_texts(train['sentence_2'])\n",
    "\n",
    "dev[['label']] = dev[['label']].astype('float32')\n",
    "dev['sentence_1'] = clean_texts(dev['sentence_1'])\n",
    "dev['sentence_2'] = clean_texts(dev['sentence_2'])\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentataion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RS, RD, RI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "import random\n",
    "\n",
    "def random_deletion(text, p=0.2):\n",
    "    mecab = Mecab()\n",
    "    tokens_with_pos = mecab.pos(text)\n",
    "    tokens = [token for token, pos in tokens_with_pos]\n",
    "    target_tags = ['IC', 'J', 'E', 'XP', 'XS']\n",
    "\n",
    "    if len(tokens) == 1:\n",
    "        return text\n",
    "    \n",
    "    remaining = [token for token, pos in tokens_with_pos if random.random() > p or not any([pos.startswith(tag) for tag in target_tags])]\n",
    "    \n",
    "    if len(remaining) == 0:\n",
    "        return random.choice(tokens)\n",
    "    \n",
    "    return ' '.join(remaining)\n",
    "\n",
    "def random_swap(text, n=1):\n",
    "    mecab = Mecab()\n",
    "    tokens_with_pos = mecab.pos(text)\n",
    "    tokens = [token for token, pos in tokens_with_pos]\n",
    "    for _ in range(n):\n",
    "        if len(tokens) >= 2:\n",
    "            idx1, idx2 = random.sample(range(len(tokens)), 2)\n",
    "            tokens[idx1], tokens[idx2] = tokens[idx2], tokens[idx1]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def random_insertion(text, n=1):\n",
    "    mecab = Mecab()\n",
    "    tokens_with_pos = mecab.pos(text)\n",
    "    tokens = [token for token, pos in tokens_with_pos]\n",
    "    for _ in range(n):\n",
    "        insert_pos = random.randint(0, len(tokens))\n",
    "        insert_word = random.choice(tokens)\n",
    "        tokens.insert(insert_pos, insert_word)\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "train_swapped = train.copy(deep=True)\n",
    "train_swapped['sentence_1'] = train_swapped['sentence_1'].apply(lambda x : random_swap(x))\n",
    "train_deleted = train.copy(deep=True)\n",
    "train_deleted['sentence_1'] = train_deleted['sentence_1'].apply(lambda x : random_deletion(x))\n",
    "train_inserted = train.copy(deep=True)\n",
    "train_inserted['sentence_1'] = train_inserted['sentence_1'].apply(lambda x : random_insertion(x))\n",
    "\n",
    "# train = pd.concat([train, train_swapped, train_deleted, train_inserted])\n",
    "train = pd.concat([train, train_deleted])\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "\n",
    "train.to_csv('data/train_augmented.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation With Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.iloc[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/STS/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 1/100....(1.00% done)\n",
      "Index: 2/100....(2.00% done)\n",
      "Index: 3/100....(3.00% done)\n",
      "Index: 4/100....(4.00% done)\n",
      "Index: 5/100....(5.00% done)\n",
      "Index: 6/100....(6.00% done)\n",
      "Index: 7/100....(7.00% done)\n",
      "Index: 8/100....(8.00% done)\n",
      "Index: 9/100....(9.00% done)\n",
      "Index: 10/100....(10.00% done)\n",
      "Index: 11/100....(11.00% done)\n",
      "Index: 12/100....(12.00% done)\n",
      "Index: 13/100....(13.00% done)\n",
      "Index: 14/100....(14.00% done)\n",
      "Index: 15/100....(15.00% done)\n",
      "Index: 16/100....(16.00% done)\n",
      "Index: 17/100....(17.00% done)\n",
      "Index: 18/100....(18.00% done)\n",
      "Index: 19/100....(19.00% done)\n",
      "Index: 20/100....(20.00% done)\n",
      "Index: 21/100....(21.00% done)\n",
      "Index: 22/100....(22.00% done)\n",
      "Index: 23/100....(23.00% done)\n",
      "Index: 24/100....(24.00% done)\n",
      "Index: 25/100....(25.00% done)\n",
      "Index: 26/100....(26.00% done)\n",
      "Index: 27/100....(27.00% done)\n",
      "Index: 28/100....(28.00% done)\n",
      "Index: 29/100....(29.00% done)\n",
      "Index: 30/100....(30.00% done)\n",
      "Index: 31/100....(31.00% done)\n",
      "Index: 32/100....(32.00% done)\n",
      "Index: 33/100....(33.00% done)\n",
      "Index: 34/100....(34.00% done)\n",
      "Index: 35/100....(35.00% done)\n",
      "Index: 36/100....(36.00% done)\n",
      "Index: 37/100....(37.00% done)\n",
      "Index: 38/100....(38.00% done)\n",
      "Index: 39/100....(39.00% done)\n",
      "Index: 40/100....(40.00% done)\n",
      "Index: 41/100....(41.00% done)\n",
      "Index: 42/100....(42.00% done)\n",
      "Index: 43/100....(43.00% done)\n",
      "Index: 44/100....(44.00% done)\n",
      "Index: 45/100....(45.00% done)\n",
      "Index: 46/100....(46.00% done)\n",
      "Index: 47/100....(47.00% done)\n",
      "Index: 48/100....(48.00% done)\n",
      "Index: 49/100....(49.00% done)\n",
      "Index: 50/100....(50.00% done)\n",
      "Index: 51/100....(51.00% done)\n",
      "Index: 52/100....(52.00% done)\n",
      "Index: 53/100....(53.00% done)\n",
      "Index: 54/100....(54.00% done)\n",
      "Index: 55/100....(55.00% done)\n",
      "Index: 56/100....(56.00% done)\n",
      "Index: 57/100....(57.00% done)\n",
      "Index: 58/100....(58.00% done)\n",
      "Index: 59/100....(59.00% done)\n",
      "Index: 60/100....(60.00% done)\n",
      "Index: 61/100....(61.00% done)\n",
      "Index: 62/100....(62.00% done)\n",
      "Index: 63/100....(63.00% done)\n",
      "Index: 64/100....(64.00% done)\n",
      "Index: 65/100....(65.00% done)\n",
      "Index: 66/100....(66.00% done)\n",
      "Index: 67/100....(67.00% done)\n",
      "Index: 68/100....(68.00% done)\n",
      "Index: 69/100....(69.00% done)\n",
      "Index: 70/100....(70.00% done)\n",
      "Index: 71/100....(71.00% done)\n",
      "Index: 72/100....(72.00% done)\n",
      "Index: 73/100....(73.00% done)\n",
      "Index: 74/100....(74.00% done)\n",
      "Index: 75/100....(75.00% done)\n",
      "Index: 76/100....(76.00% done)\n",
      "Index: 77/100....(77.00% done)\n",
      "Index: 78/100....(78.00% done)\n",
      "Index: 79/100....(79.00% done)\n",
      "Index: 80/100....(80.00% done)\n",
      "Index: 81/100....(81.00% done)\n",
      "Index: 82/100....(82.00% done)\n",
      "Index: 83/100....(83.00% done)\n",
      "Index: 84/100....(84.00% done)\n",
      "Index: 85/100....(85.00% done)\n",
      "Index: 86/100....(86.00% done)\n",
      "Index: 87/100....(87.00% done)\n",
      "Index: 88/100....(88.00% done)\n",
      "Index: 89/100....(89.00% done)\n",
      "Index: 90/100....(90.00% done)\n",
      "Index: 91/100....(91.00% done)\n",
      "Index: 92/100....(92.00% done)\n",
      "Index: 93/100....(93.00% done)\n",
      "Index: 94/100....(94.00% done)\n",
      "Index: 95/100....(95.00% done)\n",
      "Index: 96/100....(96.00% done)\n",
      "Index: 97/100....(97.00% done)\n",
      "Index: 98/100....(98.00% done)\n",
      "Index: 99/100....(99.00% done)\n",
      "Index: 100/100....(100.00% done)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"skt/kogpt2-base-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "def augment_text(text, num_augmentations=1, max_length=20):\n",
    "    augmented_texts = []\n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "    \n",
    "    for _ in range(num_augmentations):\n",
    "        output = model.generate(\n",
    "            input_ids[:,:10],\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=1,\n",
    "            no_repeat_ngram_size=2,\n",
    "            do_sample=True,\n",
    "            top_p=0.5\n",
    "        )\n",
    "        \n",
    "        augmented_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        augmented_texts.append(augmented_text)\n",
    "    \n",
    "    return augmented_texts\n",
    "\n",
    "def augment_dataframe(df, col, num_augmentations=1):\n",
    "    augmented_data = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        print(f\"Index: {_+1}/{len(df)}....({100*(_+1)/len(df):.2f}% done)\")\n",
    "        original_text = row[col]\n",
    "        augmented_texts = augment_text(original_text, num_augmentations)\n",
    "        \n",
    "        for aug_text in augmented_texts:\n",
    "            new_row = row.copy()\n",
    "            new_row[col] = aug_text\n",
    "            augmented_data.append(new_row)\n",
    "    \n",
    "    augmented_df = pd.DataFrame(augmented_data)\n",
    "    return pd.concat([df, augmented_df], ignore_index=True)\n",
    "\n",
    "train = augment_dataframe(train, 'sentence_1')\n",
    "train.tail()\n",
    "\n",
    "train.to_csv('data/train_augmented.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 1/10....(10.00% done)\n",
      "Index: 2/10....(20.00% done)\n",
      "Index: 3/10....(30.00% done)\n",
      "Index: 4/10....(40.00% done)\n",
      "Index: 5/10....(50.00% done)\n",
      "Index: 6/10....(60.00% done)\n",
      "Index: 7/10....(70.00% done)\n",
      "Index: 8/10....(80.00% done)\n",
      "Index: 9/10....(90.00% done)\n",
      "Index: 10/10....(100.00% done)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name = \"gogamza/kobart-base-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "def augment_text(text, num_augmentations=1):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=50, truncation=True)\n",
    "    \n",
    "    augmented_texts = []\n",
    "    for _ in range(num_augmentations):\n",
    "        outputs = model.generate(\n",
    "            inputs.input_ids,\n",
    "            max_length=20,\n",
    "            num_return_sequences=1,\n",
    "            do_sample=True,\n",
    "            top_p=0.5\n",
    "        )\n",
    "        \n",
    "        augmented_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        augmented_texts.append(augmented_text)\n",
    "    \n",
    "    return augmented_texts\n",
    "\n",
    "def augment_dataframe(df, col, num_augmentations=1):\n",
    "    augmented_data = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        print(f\"Index: {_+1}/{len(df)}....({100*(_+1)/len(df):.2f}% done)\")\n",
    "        original_text = row[col]\n",
    "        augmented_texts = augment_text(original_text, num_augmentations)\n",
    "        \n",
    "        for aug_text in augmented_texts:\n",
    "            new_row = row.copy()\n",
    "            new_row[col] = aug_text\n",
    "            augmented_data.append(new_row)\n",
    "    \n",
    "    augmented_df = pd.DataFrame(augmented_data)\n",
    "    return pd.concat([df, augmented_df], ignore_index=True)\n",
    "\n",
    "train = augment_dataframe(train, 'sentence_1')\n",
    "train.to_csv('data/train_augmented.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/STS/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of ElectraForMaskedLM were not initialized from the model checkpoint at beomi/KcELECTRA-base and are newly initialized: ['generator_lm_head.bias', 'generator_predictions.LayerNorm.bias', 'generator_predictions.LayerNorm.weight', 'generator_predictions.dense.bias', 'generator_predictions.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 1/100....(1.00% done)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[114], line 51\u001b[0m\n\u001b[1;32m     48\u001b[0m     augmented_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(augmented_data)\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mconcat([df, augmented_df], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 51\u001b[0m train \u001b[38;5;241m=\u001b[39m \u001b[43maugment_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msentence_1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m train\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/train_augmented.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[114], line 41\u001b[0m, in \u001b[0;36maugment_dataframe\u001b[0;34m(df, col, num_augmentations)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndex: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m....(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m100\u001b[39m\u001b[38;5;241m*\u001b[39m(_\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m% done)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     40\u001b[0m original_text \u001b[38;5;241m=\u001b[39m row[col]\n\u001b[0;32m---> 41\u001b[0m augmented_texts \u001b[38;5;241m=\u001b[39m \u001b[43maugment_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_augmentations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m aug_text \u001b[38;5;129;01min\u001b[39;00m augmented_texts:\n\u001b[1;32m     44\u001b[0m     new_row \u001b[38;5;241m=\u001b[39m row\u001b[38;5;241m.\u001b[39mcopy()\n",
      "Cell \u001b[0;32mIn[114], line 20\u001b[0m, in \u001b[0;36maugment_text\u001b[0;34m(text, num_augmentations, mask_prob)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m random\u001b[38;5;241m.\u001b[39mrandom() \u001b[38;5;241m<\u001b[39m mask_prob:\n\u001b[1;32m     18\u001b[0m         masked_tokens[i] \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mmask_token\n\u001b[0;32m---> 20\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmasked_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     22\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n",
      "File \u001b[0;32m/opt/conda/envs/STS/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2825\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m   2788\u001b[0m \u001b[38;5;129m@add_end_docstrings\u001b[39m(\n\u001b[1;32m   2789\u001b[0m     ENCODE_KWARGS_DOCSTRING,\n\u001b[1;32m   2790\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2808\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2809\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m   2810\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2811\u001b[0m \u001b[38;5;124;03m    Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.\u001b[39;00m\n\u001b[1;32m   2812\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2823\u001b[0m \u001b[38;5;124;03m            method).\u001b[39;00m\n\u001b[1;32m   2824\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2825\u001b[0m     encoded_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2827\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2828\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2829\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2830\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2831\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2832\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2833\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2834\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2835\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2837\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m encoded_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/envs/STS/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3237\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3227\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   3228\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   3229\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   3230\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3234\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3235\u001b[0m )\n\u001b[0;32m-> 3237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3238\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3239\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3240\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3241\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3242\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3243\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3244\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3245\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3247\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3248\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3249\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3250\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3255\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msplit_special_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3256\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3257\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/STS/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py:601\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_encode_plus\u001b[39m(\n\u001b[1;32m    579\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    580\u001b[0m     text: Union[TextInput, PreTokenizedInput],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    598\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    599\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchEncoding:\n\u001b[1;32m    600\u001b[0m     batched_input \u001b[38;5;241m=\u001b[39m [(text, text_pair)] \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;28;01melse\u001b[39;00m [text]\n\u001b[0;32m--> 601\u001b[0m     batched_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatched_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;66;03m# Return tensor is None, then we can remove the leading batch axis\u001b[39;00m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;66;03m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001b[39;00m\n\u001b[1;32m    624\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_tensors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_overflowing_tokens:\n",
      "File \u001b[0;32m/opt/conda/envs/STS/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py:528\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens)\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer\u001b[38;5;241m.\u001b[39mencode_special_tokens \u001b[38;5;241m!=\u001b[39m split_special_tokens:\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer\u001b[38;5;241m.\u001b[39mencode_special_tokens \u001b[38;5;241m=\u001b[39m split_special_tokens\n\u001b[0;32m--> 528\u001b[0m encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_pretokenized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;66;03m# Convert encoding to dict\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;66;03m# `Tokens` has type: Tuple[\u001b[39;00m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;66;03m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;66;03m#                       List[EncodingFast]\u001b[39;00m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;66;03m#                    ]\u001b[39;00m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;66;03m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[1;32m    540\u001b[0m tokens_and_encodings \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_encoding(\n\u001b[1;32m    542\u001b[0m         encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m encoding \u001b[38;5;129;01min\u001b[39;00m encodings\n\u001b[1;32m    552\u001b[0m ]\n",
      "\u001b[0;31mTypeError\u001b[0m: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import random\n",
    "\n",
    "model_name = \"klue/roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "def augment_text(text, num_augmentations=1, mask_prob=0.15):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    augmented_texts = []\n",
    "\n",
    "    for _ in range(num_augmentations):\n",
    "        masked_tokens = tokens.copy()\n",
    "\n",
    "        for i in range(len(masked_tokens)):\n",
    "            if random.random() < mask_prob:\n",
    "                masked_tokens[i] = tokenizer.mask_token\n",
    "        \n",
    "        inputs = tokenizer.encode(masked_tokens, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "        \n",
    "        predictions = outputs.logits.argmax(dim=-1)\n",
    "        \n",
    "        for i, token in enumerate(masked_tokens):\n",
    "            if token == tokenizer.mask_token:\n",
    "                masked_tokens[i] = tokenizer.convert_ids_to_tokens(predictions[0, i].item())\n",
    "        \n",
    "        augmented_text = tokenizer.convert_tokens_to_string(masked_tokens)\n",
    "        augmented_texts.append(augmented_text)\n",
    "    \n",
    "    return augmented_texts\n",
    "\n",
    "def augment_dataframe(df, col, num_augmentations=1):\n",
    "    augmented_data = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        print(f\"Index: {_+1}/{len(df)}....({100*(_+1)/len(df):.2f}% done)\")\n",
    "        original_text = row[col]\n",
    "        augmented_texts = augment_text(original_text, num_augmentations)\n",
    "        \n",
    "        for aug_text in augmented_texts:\n",
    "            new_row = row.copy()\n",
    "            new_row[col] = aug_text\n",
    "            augmented_data.append(new_row)\n",
    "    \n",
    "    augmented_df = pd.DataFrame(augmented_data)\n",
    "    return pd.concat([df, augmented_df], ignore_index=True)\n",
    "\n",
    "train = augment_dataframe(train, 'sentence_1')\n",
    "train.to_csv('data/train_augmented.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'BATCH_SIZE': 32,\n",
    "    'MAX_LEN': 128,\n",
    "    'LEARNING_RATE': 0.00001,\n",
    "    'EPOCHS': 10,\n",
    "    'MODEL_NAME': 'intfloat/multilingual-e5-small'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:ynz8l5dn) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▃▃▃▃▃▃▃▃▃▃▆▆▆▆▆▆▆▆▆▆██████████</td></tr><tr><td>train_loss</td><td>█▃▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▂▂▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_pearson_corr</td><td>▁▅▇▇▇▇▇█████████████████████████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>3</td></tr><tr><td>train_loss</td><td>0.20698</td></tr><tr><td>trainer/global_step</td><td>2331</td></tr><tr><td>val_loss</td><td>0.75795</td></tr><tr><td>val_pearson_corr</td><td>0.81792</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Augmentation_RD_postagging</strong> at: <a href='https://wandb.ai/kangjun205/Level1_STS/runs/ynz8l5dn' target=\"_blank\">https://wandb.ai/kangjun205/Level1_STS/runs/ynz8l5dn</a><br/> View project at: <a href='https://wandb.ai/kangjun205/Level1_STS' target=\"_blank\">https://wandb.ai/kangjun205/Level1_STS</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240919_135658-ynz8l5dn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:ynz8l5dn). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/ephemeral/level1-semantictextsimilarity-nlp-16/wandb/run-20240919_142750-k0x4gv1w</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kangjun205/Level1_STS/runs/k0x4gv1w' target=\"_blank\">Augmentation_RD_postagging</a></strong> to <a href='https://wandb.ai/kangjun205/Level1_STS' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kangjun205/Level1_STS' target=\"_blank\">https://wandb.ai/kangjun205/Level1_STS</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kangjun205/Level1_STS/runs/k0x4gv1w' target=\"_blank\">https://wandb.ai/kangjun205/Level1_STS/runs/k0x4gv1w</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/kangjun205/Level1_STS/runs/k0x4gv1w?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f6a2ddad5d0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key='dfae2bfa701b636a0c1d84ddd928a19f5e17c2f5')\n",
    "wandb.init(project=\"Level1_STS\", name = 'Augmentation_RD_postagging', config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(val_check_interval=1)` was configured so validation will run after every batch.\n",
      "/opt/conda/envs/STS/lib/python3.11/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type      | Params | Mode \n",
      "----------------------------------------------\n",
      "0 | mod     | BertModel | 117 M  | eval \n",
      "1 | dense   | Linear    | 385    | train\n",
      "2 | sigmoid | Sigmoid   | 0      | train\n",
      "----------------------------------------------\n",
      "117 M     Trainable params\n",
      "0         Non-trainable params\n",
      "117 M     Total params\n",
      "470.617   Total estimated model params size (MB)\n",
      "2         Modules in train mode\n",
      "228       Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/STS/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/STS/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 583/583 [06:24<00:00,  1.52it/s, v_num=gv1w]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 583/583 [06:36<00:00,  1.47it/s, v_num=gv1w]\n"
     ]
    }
   ],
   "source": [
    "now_min = datetime.datetime.now().strftime('%d%H%M')\n",
    "now_sec = datetime.datetime.now().strftime('%d%H%M%S')\n",
    "\n",
    "tokenizer = get_tokenizer(config['MODEL_NAME'])\n",
    "dataloader = TextDataLoader(\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=config['MAX_LEN'],\n",
    "    train_data=train,\n",
    "    dev_data=dev,\n",
    "    truncation=True,\n",
    "    batch_size=config['BATCH_SIZE']\n",
    ")\n",
    "model = STSModel(config)\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath='saved',\n",
    "    filename=f'best-model-{now_sec}',\n",
    "    save_top_k=3,\n",
    "    monitor='val_loss',\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "wandb_checkpoint_callback = WandbCheckpointCallback(top_k=3)\n",
    "\n",
    "model_name = config['MODEL_NAME']\n",
    "run_name = f'{model_name}-{now_min}'\n",
    "wandb_logger = WandbLogger(name = run_name, project=\"Level1-STS\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    max_epochs=config['EPOCHS'],\n",
    "    val_check_interval=1,\n",
    "    callbacks=[early_stop_callback, checkpoint_callback],\n",
    "    logger = wandb_logger\n",
    "    )\n",
    "\n",
    "trainer.fit(model, datamodule=dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/envs/STS/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/opt/conda/envs/STS/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 35/35 [00:01<00:00, 34.40it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'predictions': tensor([[3.2177],\n",
       "          [4.1740],\n",
       "          [2.7560],\n",
       "          [0.4909],\n",
       "          [3.8552],\n",
       "          [3.5434],\n",
       "          [4.0969],\n",
       "          [1.1194],\n",
       "          [0.4112],\n",
       "          [4.1289],\n",
       "          [2.5467],\n",
       "          [2.9074],\n",
       "          [2.4398],\n",
       "          [2.2452],\n",
       "          [2.3925],\n",
       "          [4.2778],\n",
       "          [4.2575],\n",
       "          [1.5046],\n",
       "          [4.1177],\n",
       "          [4.1954],\n",
       "          [1.9899],\n",
       "          [3.0071],\n",
       "          [1.6489],\n",
       "          [0.4607],\n",
       "          [1.3348],\n",
       "          [1.9117],\n",
       "          [1.6809],\n",
       "          [1.5180],\n",
       "          [3.1211],\n",
       "          [3.4885],\n",
       "          [1.7993],\n",
       "          [1.0880]])},\n",
       " {'predictions': tensor([[3.0053],\n",
       "          [2.0897],\n",
       "          [3.2302],\n",
       "          [1.5694],\n",
       "          [2.4436],\n",
       "          [0.3814],\n",
       "          [0.6560],\n",
       "          [3.1371],\n",
       "          [0.7263],\n",
       "          [3.6687],\n",
       "          [0.3816],\n",
       "          [1.7599],\n",
       "          [2.9931],\n",
       "          [4.2596],\n",
       "          [1.0648],\n",
       "          [3.5591],\n",
       "          [0.9654],\n",
       "          [3.5371],\n",
       "          [0.4061],\n",
       "          [0.8465],\n",
       "          [3.0007],\n",
       "          [3.4948],\n",
       "          [3.2959],\n",
       "          [1.6162],\n",
       "          [3.6227],\n",
       "          [1.4306],\n",
       "          [3.3918],\n",
       "          [3.5799],\n",
       "          [3.4635],\n",
       "          [1.3351],\n",
       "          [1.3535],\n",
       "          [1.6574]])},\n",
       " {'predictions': tensor([[2.8704],\n",
       "          [3.7926],\n",
       "          [1.6266],\n",
       "          [0.8979],\n",
       "          [2.0413],\n",
       "          [3.4002],\n",
       "          [0.3758],\n",
       "          [2.0838],\n",
       "          [3.8942],\n",
       "          [3.3117],\n",
       "          [0.3745],\n",
       "          [1.4618],\n",
       "          [3.2790],\n",
       "          [1.9721],\n",
       "          [1.7976],\n",
       "          [3.9162],\n",
       "          [1.6406],\n",
       "          [2.6821],\n",
       "          [2.7847],\n",
       "          [3.3082],\n",
       "          [1.2569],\n",
       "          [4.2766],\n",
       "          [3.4979],\n",
       "          [1.1878],\n",
       "          [3.8671],\n",
       "          [4.0572],\n",
       "          [3.6926],\n",
       "          [1.8781],\n",
       "          [0.9004],\n",
       "          [2.4593],\n",
       "          [4.1191],\n",
       "          [1.5655]])},\n",
       " {'predictions': tensor([[2.9080],\n",
       "          [3.1150],\n",
       "          [0.4347],\n",
       "          [2.4050],\n",
       "          [2.9287],\n",
       "          [4.1146],\n",
       "          [3.5943],\n",
       "          [2.5525],\n",
       "          [0.6877],\n",
       "          [0.4925],\n",
       "          [3.6346],\n",
       "          [3.5477],\n",
       "          [2.2563],\n",
       "          [3.9524],\n",
       "          [3.6782],\n",
       "          [0.3884],\n",
       "          [0.3666],\n",
       "          [1.9907],\n",
       "          [1.1777],\n",
       "          [2.2598],\n",
       "          [0.5263],\n",
       "          [4.2656],\n",
       "          [3.0558],\n",
       "          [3.4181],\n",
       "          [4.2121],\n",
       "          [2.4425],\n",
       "          [1.9640],\n",
       "          [2.7661],\n",
       "          [2.4884],\n",
       "          [1.6074],\n",
       "          [3.8919],\n",
       "          [3.5022]])},\n",
       " {'predictions': tensor([[4.1463],\n",
       "          [4.2456],\n",
       "          [2.7616],\n",
       "          [3.6909],\n",
       "          [4.1957],\n",
       "          [2.6431],\n",
       "          [4.2484],\n",
       "          [3.6687],\n",
       "          [1.5572],\n",
       "          [3.3294],\n",
       "          [2.1055],\n",
       "          [3.4059],\n",
       "          [3.9035],\n",
       "          [1.3453],\n",
       "          [2.3040],\n",
       "          [4.2003],\n",
       "          [0.9753],\n",
       "          [0.3857],\n",
       "          [0.8799],\n",
       "          [1.4916],\n",
       "          [1.1849],\n",
       "          [3.9206],\n",
       "          [1.3153],\n",
       "          [4.2550],\n",
       "          [1.8460],\n",
       "          [3.3393],\n",
       "          [0.8195],\n",
       "          [0.4127],\n",
       "          [4.1754],\n",
       "          [4.1920],\n",
       "          [4.2702],\n",
       "          [1.2539]])},\n",
       " {'predictions': tensor([[4.1110],\n",
       "          [1.0395],\n",
       "          [3.4132],\n",
       "          [2.8602],\n",
       "          [2.3850],\n",
       "          [4.2728],\n",
       "          [1.1186],\n",
       "          [3.8244],\n",
       "          [1.6718],\n",
       "          [3.4714],\n",
       "          [2.8869],\n",
       "          [3.8346],\n",
       "          [4.1506],\n",
       "          [1.5043],\n",
       "          [2.0570],\n",
       "          [2.8193],\n",
       "          [0.3640],\n",
       "          [3.6078],\n",
       "          [1.5399],\n",
       "          [2.8236],\n",
       "          [3.0275],\n",
       "          [2.5397],\n",
       "          [2.2557],\n",
       "          [2.4621],\n",
       "          [3.7129],\n",
       "          [4.0328],\n",
       "          [3.9097],\n",
       "          [3.5963],\n",
       "          [0.4092],\n",
       "          [4.1121],\n",
       "          [1.0341],\n",
       "          [1.6343]])},\n",
       " {'predictions': tensor([[1.2317],\n",
       "          [1.6660],\n",
       "          [3.1580],\n",
       "          [0.5609],\n",
       "          [1.6936],\n",
       "          [3.5269],\n",
       "          [3.8971],\n",
       "          [0.7713],\n",
       "          [2.9619],\n",
       "          [2.7924],\n",
       "          [4.0360],\n",
       "          [1.1860],\n",
       "          [2.2212],\n",
       "          [2.0003],\n",
       "          [2.1083],\n",
       "          [2.2419],\n",
       "          [1.1016],\n",
       "          [2.7839],\n",
       "          [1.0746],\n",
       "          [3.2532],\n",
       "          [0.9126],\n",
       "          [1.9018],\n",
       "          [3.9775],\n",
       "          [2.9178],\n",
       "          [3.7509],\n",
       "          [1.0342],\n",
       "          [2.3525],\n",
       "          [0.3604],\n",
       "          [3.8494],\n",
       "          [2.8706],\n",
       "          [3.6189],\n",
       "          [2.1419]])},\n",
       " {'predictions': tensor([[4.1848],\n",
       "          [3.7909],\n",
       "          [3.8226],\n",
       "          [2.4593],\n",
       "          [2.6949],\n",
       "          [0.4428],\n",
       "          [1.3928],\n",
       "          [1.2312],\n",
       "          [3.4521],\n",
       "          [2.5292],\n",
       "          [0.3658],\n",
       "          [1.6245],\n",
       "          [2.4570],\n",
       "          [2.4161],\n",
       "          [2.1448],\n",
       "          [1.0288],\n",
       "          [1.3434],\n",
       "          [0.8611],\n",
       "          [0.4510],\n",
       "          [3.4798],\n",
       "          [1.7433],\n",
       "          [3.9551],\n",
       "          [2.0588],\n",
       "          [4.1310],\n",
       "          [3.6453],\n",
       "          [2.5613],\n",
       "          [1.0391],\n",
       "          [1.1552],\n",
       "          [0.3847],\n",
       "          [3.4072],\n",
       "          [1.3568],\n",
       "          [4.2693]])},\n",
       " {'predictions': tensor([[1.9272],\n",
       "          [2.4793],\n",
       "          [4.2082],\n",
       "          [1.1931],\n",
       "          [4.1126],\n",
       "          [4.2539],\n",
       "          [4.2031],\n",
       "          [4.0562],\n",
       "          [4.0566],\n",
       "          [3.9939],\n",
       "          [4.0115],\n",
       "          [2.3350],\n",
       "          [1.1346],\n",
       "          [0.4202],\n",
       "          [4.0329],\n",
       "          [2.9544],\n",
       "          [0.8802],\n",
       "          [4.2360],\n",
       "          [3.4040],\n",
       "          [4.0920],\n",
       "          [2.4762],\n",
       "          [0.8623],\n",
       "          [2.7486],\n",
       "          [4.2327],\n",
       "          [1.8473],\n",
       "          [3.8876],\n",
       "          [3.8518],\n",
       "          [0.5504],\n",
       "          [3.8941],\n",
       "          [2.1588],\n",
       "          [3.3184],\n",
       "          [3.8266]])},\n",
       " {'predictions': tensor([[0.3838],\n",
       "          [0.4101],\n",
       "          [0.7191],\n",
       "          [0.3722],\n",
       "          [3.5572],\n",
       "          [1.1127],\n",
       "          [2.7460],\n",
       "          [2.9800],\n",
       "          [2.1009],\n",
       "          [0.3682],\n",
       "          [4.2744],\n",
       "          [1.4572],\n",
       "          [4.2245],\n",
       "          [3.7748],\n",
       "          [1.4064],\n",
       "          [2.3984],\n",
       "          [1.8524],\n",
       "          [1.7327],\n",
       "          [3.0194],\n",
       "          [4.2416],\n",
       "          [0.6070],\n",
       "          [1.7451],\n",
       "          [4.0540],\n",
       "          [2.5090],\n",
       "          [0.6595],\n",
       "          [4.0193],\n",
       "          [3.0956],\n",
       "          [3.2637],\n",
       "          [3.8847],\n",
       "          [0.4114],\n",
       "          [3.7031],\n",
       "          [1.8619]])},\n",
       " {'predictions': tensor([[0.5161],\n",
       "          [1.0991],\n",
       "          [3.9872],\n",
       "          [0.3810],\n",
       "          [0.9430],\n",
       "          [4.1853],\n",
       "          [3.4163],\n",
       "          [2.3407],\n",
       "          [3.8762],\n",
       "          [0.6681],\n",
       "          [2.1653],\n",
       "          [4.0825],\n",
       "          [3.8022],\n",
       "          [4.1772],\n",
       "          [1.1559],\n",
       "          [4.2398],\n",
       "          [4.0013],\n",
       "          [0.3852],\n",
       "          [4.0981],\n",
       "          [2.6380],\n",
       "          [0.7500],\n",
       "          [4.2968],\n",
       "          [2.5948],\n",
       "          [2.0712],\n",
       "          [4.1931],\n",
       "          [4.0308],\n",
       "          [1.5947],\n",
       "          [2.8229],\n",
       "          [2.4847],\n",
       "          [2.5927],\n",
       "          [4.1943],\n",
       "          [2.9331]])},\n",
       " {'predictions': tensor([[2.8428],\n",
       "          [0.4248],\n",
       "          [0.3868],\n",
       "          [2.3299],\n",
       "          [3.2850],\n",
       "          [1.5860],\n",
       "          [1.0995],\n",
       "          [3.7163],\n",
       "          [3.1502],\n",
       "          [1.9145],\n",
       "          [3.2775],\n",
       "          [0.4680],\n",
       "          [3.5296],\n",
       "          [4.2640],\n",
       "          [2.7217],\n",
       "          [2.5643],\n",
       "          [0.9946],\n",
       "          [0.6200],\n",
       "          [1.0292],\n",
       "          [3.7241],\n",
       "          [1.4460],\n",
       "          [0.3597],\n",
       "          [1.6703],\n",
       "          [4.2441],\n",
       "          [1.1945],\n",
       "          [3.0947],\n",
       "          [4.2373],\n",
       "          [4.1839],\n",
       "          [3.0888],\n",
       "          [0.5926],\n",
       "          [1.9483],\n",
       "          [2.8867]])},\n",
       " {'predictions': tensor([[0.5276],\n",
       "          [0.9261],\n",
       "          [3.0573],\n",
       "          [1.4071],\n",
       "          [3.2618],\n",
       "          [2.6773],\n",
       "          [3.4614],\n",
       "          [1.9807],\n",
       "          [1.4965],\n",
       "          [4.2137],\n",
       "          [3.8870],\n",
       "          [2.9609],\n",
       "          [0.8301],\n",
       "          [3.4484],\n",
       "          [1.5876],\n",
       "          [1.0958],\n",
       "          [1.8621],\n",
       "          [2.7843],\n",
       "          [1.8823],\n",
       "          [4.2183],\n",
       "          [4.2068],\n",
       "          [1.1055],\n",
       "          [4.0119],\n",
       "          [2.3709],\n",
       "          [3.1043],\n",
       "          [4.2727],\n",
       "          [3.3172],\n",
       "          [1.2817],\n",
       "          [0.4149],\n",
       "          [3.9784],\n",
       "          [3.3624],\n",
       "          [1.2629]])},\n",
       " {'predictions': tensor([[2.5804],\n",
       "          [3.1034],\n",
       "          [4.2288],\n",
       "          [2.3076],\n",
       "          [4.1575],\n",
       "          [2.3033],\n",
       "          [3.6692],\n",
       "          [3.8150],\n",
       "          [2.8196],\n",
       "          [2.3636],\n",
       "          [1.7701],\n",
       "          [4.2252],\n",
       "          [4.0430],\n",
       "          [3.0590],\n",
       "          [0.6431],\n",
       "          [0.3775],\n",
       "          [1.5189],\n",
       "          [1.7482],\n",
       "          [0.4061],\n",
       "          [2.3986],\n",
       "          [1.6707],\n",
       "          [1.7282],\n",
       "          [2.2667],\n",
       "          [1.4037],\n",
       "          [0.4122],\n",
       "          [2.7718],\n",
       "          [3.1969],\n",
       "          [2.7637],\n",
       "          [3.3984],\n",
       "          [0.5046],\n",
       "          [3.6281],\n",
       "          [1.2751]])},\n",
       " {'predictions': tensor([[4.1071],\n",
       "          [4.0727],\n",
       "          [1.9898],\n",
       "          [1.0031],\n",
       "          [2.8623],\n",
       "          [2.3149],\n",
       "          [1.7596],\n",
       "          [0.4995],\n",
       "          [1.4041],\n",
       "          [3.0289],\n",
       "          [2.1080],\n",
       "          [1.6360],\n",
       "          [0.3868],\n",
       "          [3.8529],\n",
       "          [4.2022],\n",
       "          [3.0985],\n",
       "          [2.5423],\n",
       "          [0.3915],\n",
       "          [2.2834],\n",
       "          [1.3918],\n",
       "          [2.1577],\n",
       "          [1.8439],\n",
       "          [2.2760],\n",
       "          [0.3823],\n",
       "          [2.0259],\n",
       "          [4.2044],\n",
       "          [3.4200],\n",
       "          [0.7216],\n",
       "          [4.0718],\n",
       "          [1.2287],\n",
       "          [1.6718],\n",
       "          [2.5446]])},\n",
       " {'predictions': tensor([[3.8158],\n",
       "          [0.8400],\n",
       "          [4.2465],\n",
       "          [4.2614],\n",
       "          [2.0905],\n",
       "          [2.5120],\n",
       "          [1.6651],\n",
       "          [0.8181],\n",
       "          [0.5103],\n",
       "          [1.1208],\n",
       "          [2.9527],\n",
       "          [1.3385],\n",
       "          [1.6477],\n",
       "          [0.3937],\n",
       "          [4.1630],\n",
       "          [1.2135],\n",
       "          [4.1693],\n",
       "          [4.2729],\n",
       "          [3.4437],\n",
       "          [2.9513],\n",
       "          [1.0150],\n",
       "          [4.2000],\n",
       "          [3.8109],\n",
       "          [3.0162],\n",
       "          [4.0710],\n",
       "          [1.0667],\n",
       "          [1.0696],\n",
       "          [0.5822],\n",
       "          [0.3826],\n",
       "          [3.8643],\n",
       "          [1.2467],\n",
       "          [1.6431]])},\n",
       " {'predictions': tensor([[0.4136],\n",
       "          [1.4294],\n",
       "          [1.3472],\n",
       "          [0.9429],\n",
       "          [1.3234],\n",
       "          [1.8937],\n",
       "          [3.4429],\n",
       "          [1.2218],\n",
       "          [0.8832],\n",
       "          [1.5023],\n",
       "          [0.7227],\n",
       "          [2.8492],\n",
       "          [2.5112],\n",
       "          [2.5948],\n",
       "          [2.2152],\n",
       "          [1.5686],\n",
       "          [2.0498],\n",
       "          [3.7640],\n",
       "          [3.9515],\n",
       "          [2.6678],\n",
       "          [0.3723],\n",
       "          [2.1659],\n",
       "          [1.6846],\n",
       "          [4.0836],\n",
       "          [2.7417],\n",
       "          [2.2010],\n",
       "          [1.8691],\n",
       "          [1.7039],\n",
       "          [1.8682],\n",
       "          [4.2733],\n",
       "          [2.6174],\n",
       "          [0.8301]])},\n",
       " {'predictions': tensor([[0.7843],\n",
       "          [2.7315],\n",
       "          [4.2827],\n",
       "          [4.0963],\n",
       "          [1.4760],\n",
       "          [2.3105],\n",
       "          [1.2201],\n",
       "          [0.5028],\n",
       "          [2.5314],\n",
       "          [3.0717],\n",
       "          [4.0346],\n",
       "          [3.4780],\n",
       "          [1.6200],\n",
       "          [2.1954],\n",
       "          [2.3967],\n",
       "          [2.5981],\n",
       "          [4.2081],\n",
       "          [4.0256],\n",
       "          [0.6053],\n",
       "          [2.7708],\n",
       "          [0.4559],\n",
       "          [3.4351],\n",
       "          [3.8056],\n",
       "          [2.7029],\n",
       "          [3.3001],\n",
       "          [4.2837],\n",
       "          [2.6884],\n",
       "          [2.2536],\n",
       "          [0.4771],\n",
       "          [2.0365],\n",
       "          [1.9837],\n",
       "          [3.3003]])},\n",
       " {'predictions': tensor([[4.2562],\n",
       "          [1.0747],\n",
       "          [2.2461],\n",
       "          [2.3998],\n",
       "          [3.4649],\n",
       "          [0.3752],\n",
       "          [1.0513],\n",
       "          [4.0949],\n",
       "          [1.3925],\n",
       "          [4.2368],\n",
       "          [3.0094],\n",
       "          [0.9477],\n",
       "          [3.4549],\n",
       "          [0.6923],\n",
       "          [3.7307],\n",
       "          [3.3909],\n",
       "          [1.2880],\n",
       "          [4.1917],\n",
       "          [3.2493],\n",
       "          [0.6190],\n",
       "          [4.1821],\n",
       "          [3.8034],\n",
       "          [2.6486],\n",
       "          [2.3400],\n",
       "          [3.1225],\n",
       "          [3.9341],\n",
       "          [4.2383],\n",
       "          [2.8222],\n",
       "          [4.2208],\n",
       "          [0.5354],\n",
       "          [1.3176],\n",
       "          [3.1965]])},\n",
       " {'predictions': tensor([[0.4212],\n",
       "          [2.8503],\n",
       "          [3.8256],\n",
       "          [2.4854],\n",
       "          [3.8196],\n",
       "          [2.0212],\n",
       "          [0.4800],\n",
       "          [3.4200],\n",
       "          [3.0845],\n",
       "          [2.4338],\n",
       "          [3.6190],\n",
       "          [1.1962],\n",
       "          [0.6558],\n",
       "          [3.1806],\n",
       "          [2.7293],\n",
       "          [3.8759],\n",
       "          [4.0771],\n",
       "          [0.6181],\n",
       "          [3.7549],\n",
       "          [4.2322],\n",
       "          [3.0030],\n",
       "          [2.1439],\n",
       "          [0.7256],\n",
       "          [2.2817],\n",
       "          [1.2492],\n",
       "          [3.8484],\n",
       "          [0.8981],\n",
       "          [2.8260],\n",
       "          [2.5021],\n",
       "          [0.4334],\n",
       "          [1.1922],\n",
       "          [3.7867]])},\n",
       " {'predictions': tensor([[2.6778],\n",
       "          [3.7354],\n",
       "          [1.3589],\n",
       "          [2.4186],\n",
       "          [2.0643],\n",
       "          [0.5401],\n",
       "          [3.7070],\n",
       "          [3.1561],\n",
       "          [0.9125],\n",
       "          [2.6252],\n",
       "          [3.5460],\n",
       "          [2.2636],\n",
       "          [3.7770],\n",
       "          [1.6996],\n",
       "          [2.2211],\n",
       "          [2.6401],\n",
       "          [4.1737],\n",
       "          [1.6381],\n",
       "          [1.7027],\n",
       "          [4.2104],\n",
       "          [3.9769],\n",
       "          [0.3900],\n",
       "          [4.0828],\n",
       "          [3.8682],\n",
       "          [1.7035],\n",
       "          [0.8431],\n",
       "          [0.9372],\n",
       "          [2.4917],\n",
       "          [2.0272],\n",
       "          [2.6569],\n",
       "          [2.0227],\n",
       "          [3.1631]])},\n",
       " {'predictions': tensor([[2.7116],\n",
       "          [4.1444],\n",
       "          [1.4717],\n",
       "          [4.1118],\n",
       "          [3.6252],\n",
       "          [2.2905],\n",
       "          [4.1288],\n",
       "          [4.1894],\n",
       "          [0.6247],\n",
       "          [1.5130],\n",
       "          [0.8644],\n",
       "          [3.7766],\n",
       "          [3.3102],\n",
       "          [1.9534],\n",
       "          [3.5463],\n",
       "          [4.2774],\n",
       "          [4.2002],\n",
       "          [2.0150],\n",
       "          [3.9645],\n",
       "          [2.9862],\n",
       "          [0.3923],\n",
       "          [4.1987],\n",
       "          [2.3340],\n",
       "          [3.7502],\n",
       "          [2.8282],\n",
       "          [3.8563],\n",
       "          [3.9360],\n",
       "          [0.5128],\n",
       "          [4.1348],\n",
       "          [0.3957],\n",
       "          [1.4975],\n",
       "          [0.5520]])},\n",
       " {'predictions': tensor([[2.2405],\n",
       "          [3.2933],\n",
       "          [1.3120],\n",
       "          [3.8411],\n",
       "          [2.7992],\n",
       "          [4.1316],\n",
       "          [0.4669],\n",
       "          [2.1187],\n",
       "          [4.1256],\n",
       "          [2.7419],\n",
       "          [2.5229],\n",
       "          [0.3715],\n",
       "          [2.7446],\n",
       "          [4.2125],\n",
       "          [4.1778],\n",
       "          [3.2349],\n",
       "          [4.0683],\n",
       "          [4.1595],\n",
       "          [0.5502],\n",
       "          [0.3725],\n",
       "          [1.6988],\n",
       "          [1.0041],\n",
       "          [2.1876],\n",
       "          [3.8685],\n",
       "          [3.6121],\n",
       "          [3.9458],\n",
       "          [1.5362],\n",
       "          [2.2268],\n",
       "          [2.2280],\n",
       "          [2.0658],\n",
       "          [1.4434],\n",
       "          [4.2621]])},\n",
       " {'predictions': tensor([[3.1159],\n",
       "          [4.2120],\n",
       "          [1.5369],\n",
       "          [2.3087],\n",
       "          [2.6312],\n",
       "          [1.4679],\n",
       "          [2.6908],\n",
       "          [3.5998],\n",
       "          [3.4619],\n",
       "          [2.2048],\n",
       "          [3.3434],\n",
       "          [3.1406],\n",
       "          [3.9163],\n",
       "          [2.0511],\n",
       "          [4.1601],\n",
       "          [3.4968],\n",
       "          [0.3958],\n",
       "          [2.4484],\n",
       "          [0.7927],\n",
       "          [2.8419],\n",
       "          [3.0644],\n",
       "          [2.0503],\n",
       "          [3.9991],\n",
       "          [1.0318],\n",
       "          [1.9160],\n",
       "          [1.9435],\n",
       "          [3.9245],\n",
       "          [4.2439],\n",
       "          [3.6408],\n",
       "          [1.0701],\n",
       "          [1.7753],\n",
       "          [0.7502]])},\n",
       " {'predictions': tensor([[4.0135],\n",
       "          [1.2745],\n",
       "          [4.0796],\n",
       "          [4.2816],\n",
       "          [1.9673],\n",
       "          [0.9807],\n",
       "          [0.4161],\n",
       "          [3.3058],\n",
       "          [4.1351],\n",
       "          [3.6860],\n",
       "          [2.3887],\n",
       "          [2.1038],\n",
       "          [1.2693],\n",
       "          [0.8358],\n",
       "          [2.7344],\n",
       "          [2.8150],\n",
       "          [4.0953],\n",
       "          [2.2578],\n",
       "          [1.0110],\n",
       "          [2.3426],\n",
       "          [1.4131],\n",
       "          [0.4561],\n",
       "          [1.3794],\n",
       "          [2.4127],\n",
       "          [2.4468],\n",
       "          [2.6273],\n",
       "          [2.3218],\n",
       "          [4.2000],\n",
       "          [2.9109],\n",
       "          [3.0234],\n",
       "          [0.4087],\n",
       "          [2.6341]])},\n",
       " {'predictions': tensor([[0.5913],\n",
       "          [2.8792],\n",
       "          [3.7799],\n",
       "          [1.9613],\n",
       "          [2.9272],\n",
       "          [0.5659],\n",
       "          [1.4222],\n",
       "          [1.8194],\n",
       "          [3.4180],\n",
       "          [3.5479],\n",
       "          [3.7243],\n",
       "          [1.5545],\n",
       "          [1.5281],\n",
       "          [3.7181],\n",
       "          [0.7939],\n",
       "          [1.8598],\n",
       "          [1.6806],\n",
       "          [1.5506],\n",
       "          [4.1436],\n",
       "          [4.1400],\n",
       "          [1.5613],\n",
       "          [1.9218],\n",
       "          [1.5073],\n",
       "          [2.8745],\n",
       "          [0.6140],\n",
       "          [1.5801],\n",
       "          [3.9462],\n",
       "          [4.2300],\n",
       "          [3.4638],\n",
       "          [2.3217],\n",
       "          [1.4404],\n",
       "          [1.2674]])},\n",
       " {'predictions': tensor([[3.2385],\n",
       "          [1.8608],\n",
       "          [2.3444],\n",
       "          [2.3904],\n",
       "          [4.1888],\n",
       "          [3.0798],\n",
       "          [3.4852],\n",
       "          [0.8281],\n",
       "          [1.4657],\n",
       "          [3.8644],\n",
       "          [3.5704],\n",
       "          [4.2132],\n",
       "          [1.4421],\n",
       "          [2.9802],\n",
       "          [4.2348],\n",
       "          [4.2628],\n",
       "          [3.7833],\n",
       "          [1.2769],\n",
       "          [1.7982],\n",
       "          [2.4213],\n",
       "          [3.0803],\n",
       "          [1.3083],\n",
       "          [4.1483],\n",
       "          [3.2643],\n",
       "          [3.3415],\n",
       "          [1.2311],\n",
       "          [1.3251],\n",
       "          [0.3648],\n",
       "          [4.2014],\n",
       "          [1.6936],\n",
       "          [3.3443],\n",
       "          [4.2453]])},\n",
       " {'predictions': tensor([[4.2335],\n",
       "          [2.5446],\n",
       "          [1.6893],\n",
       "          [0.4734],\n",
       "          [3.6019],\n",
       "          [1.2510],\n",
       "          [4.1792],\n",
       "          [3.1681],\n",
       "          [0.7781],\n",
       "          [3.1254],\n",
       "          [2.5452],\n",
       "          [0.4921],\n",
       "          [4.1234],\n",
       "          [0.9480],\n",
       "          [2.1748],\n",
       "          [2.1990],\n",
       "          [4.2398],\n",
       "          [4.2331],\n",
       "          [1.6330],\n",
       "          [3.3205],\n",
       "          [3.5507],\n",
       "          [4.0099],\n",
       "          [2.0376],\n",
       "          [3.0414],\n",
       "          [2.2616],\n",
       "          [3.8244],\n",
       "          [3.4122],\n",
       "          [2.5481],\n",
       "          [4.0430],\n",
       "          [3.9588],\n",
       "          [1.7146],\n",
       "          [3.3476]])},\n",
       " {'predictions': tensor([[1.9611],\n",
       "          [2.2895],\n",
       "          [0.3982],\n",
       "          [3.9557],\n",
       "          [0.6102],\n",
       "          [1.1870],\n",
       "          [1.0691],\n",
       "          [3.5953],\n",
       "          [3.3078],\n",
       "          [2.7017],\n",
       "          [3.1318],\n",
       "          [1.9116],\n",
       "          [0.6188],\n",
       "          [4.1897],\n",
       "          [4.2100],\n",
       "          [2.7908],\n",
       "          [1.7836],\n",
       "          [4.0398],\n",
       "          [0.7809],\n",
       "          [2.8489],\n",
       "          [3.7302],\n",
       "          [3.4839],\n",
       "          [4.1784],\n",
       "          [4.1160],\n",
       "          [3.6435],\n",
       "          [2.8184],\n",
       "          [2.2852],\n",
       "          [4.2602],\n",
       "          [4.2912],\n",
       "          [0.8230],\n",
       "          [2.0947],\n",
       "          [4.2036]])},\n",
       " {'predictions': tensor([[4.0139],\n",
       "          [4.0860],\n",
       "          [4.1046],\n",
       "          [2.1578],\n",
       "          [2.3128],\n",
       "          [2.4719],\n",
       "          [3.2615],\n",
       "          [0.3745],\n",
       "          [1.9670],\n",
       "          [3.9214],\n",
       "          [1.6048],\n",
       "          [3.1559],\n",
       "          [4.2286],\n",
       "          [3.8872],\n",
       "          [3.2781],\n",
       "          [1.5042],\n",
       "          [3.7509],\n",
       "          [0.3715],\n",
       "          [0.5534],\n",
       "          [2.8596],\n",
       "          [4.2394],\n",
       "          [3.5769],\n",
       "          [3.7045],\n",
       "          [3.2390],\n",
       "          [1.8020],\n",
       "          [4.1229],\n",
       "          [3.1109],\n",
       "          [4.2108],\n",
       "          [2.0451],\n",
       "          [2.0896],\n",
       "          [1.0848],\n",
       "          [1.6444]])},\n",
       " {'predictions': tensor([[3.9306],\n",
       "          [3.3275],\n",
       "          [3.0149],\n",
       "          [4.2701],\n",
       "          [3.6797],\n",
       "          [1.9440],\n",
       "          [1.0556],\n",
       "          [2.3623],\n",
       "          [3.4895],\n",
       "          [0.4025],\n",
       "          [1.8710],\n",
       "          [2.9437],\n",
       "          [0.5430],\n",
       "          [1.4259],\n",
       "          [2.4734],\n",
       "          [2.7198],\n",
       "          [1.8590],\n",
       "          [0.8664],\n",
       "          [4.1123],\n",
       "          [2.4712],\n",
       "          [2.1483],\n",
       "          [3.4426],\n",
       "          [4.1901],\n",
       "          [1.3325],\n",
       "          [4.0691],\n",
       "          [1.7624],\n",
       "          [2.4607],\n",
       "          [1.3754],\n",
       "          [4.1544],\n",
       "          [2.7632],\n",
       "          [1.9984],\n",
       "          [4.2390]])},\n",
       " {'predictions': tensor([[3.9794],\n",
       "          [1.2093],\n",
       "          [1.8570],\n",
       "          [1.4269],\n",
       "          [2.2023],\n",
       "          [3.6695],\n",
       "          [0.6833],\n",
       "          [3.5516],\n",
       "          [4.1491],\n",
       "          [4.1789],\n",
       "          [2.0931],\n",
       "          [3.8954],\n",
       "          [4.2879],\n",
       "          [2.8056],\n",
       "          [4.0698],\n",
       "          [0.4839],\n",
       "          [0.7605],\n",
       "          [1.9819],\n",
       "          [0.3911],\n",
       "          [4.2870],\n",
       "          [1.4363],\n",
       "          [3.4170],\n",
       "          [1.3622],\n",
       "          [3.3058],\n",
       "          [3.6403],\n",
       "          [4.1245],\n",
       "          [0.9128],\n",
       "          [3.0858],\n",
       "          [3.3058],\n",
       "          [1.4295],\n",
       "          [1.9449],\n",
       "          [3.3154]])},\n",
       " {'predictions': tensor([[3.9004],\n",
       "          [3.6842],\n",
       "          [2.0005],\n",
       "          [4.0082],\n",
       "          [2.2394],\n",
       "          [4.1689],\n",
       "          [1.4970],\n",
       "          [3.8539],\n",
       "          [3.2494],\n",
       "          [4.2811],\n",
       "          [3.3994],\n",
       "          [1.8752],\n",
       "          [2.2865],\n",
       "          [1.3789],\n",
       "          [2.2353],\n",
       "          [4.2525],\n",
       "          [3.7602],\n",
       "          [1.2865],\n",
       "          [3.4907],\n",
       "          [3.3693],\n",
       "          [1.6929],\n",
       "          [4.2147],\n",
       "          [4.2381],\n",
       "          [3.6763],\n",
       "          [2.7799],\n",
       "          [1.5474],\n",
       "          [4.1958],\n",
       "          [4.1624],\n",
       "          [1.5884],\n",
       "          [0.3651],\n",
       "          [1.2520],\n",
       "          [3.6755]])},\n",
       " {'predictions': tensor([[3.9000],\n",
       "          [1.1406],\n",
       "          [1.3577],\n",
       "          [3.7893],\n",
       "          [3.8602],\n",
       "          [1.4438],\n",
       "          [4.2693],\n",
       "          [3.4896],\n",
       "          [0.8422],\n",
       "          [0.4318],\n",
       "          [2.7910],\n",
       "          [3.7532],\n",
       "          [4.1857],\n",
       "          [1.5002],\n",
       "          [3.4619],\n",
       "          [0.7297],\n",
       "          [4.2405],\n",
       "          [2.0522],\n",
       "          [3.5561],\n",
       "          [2.2737],\n",
       "          [3.8662],\n",
       "          [0.4805],\n",
       "          [3.5821],\n",
       "          [3.0313],\n",
       "          [4.2137],\n",
       "          [2.9175],\n",
       "          [3.1282],\n",
       "          [2.6605],\n",
       "          [2.0333],\n",
       "          [2.9291],\n",
       "          [2.1732],\n",
       "          [2.5221]])},\n",
       " {'predictions': tensor([[3.5313],\n",
       "          [3.7272],\n",
       "          [0.4097],\n",
       "          [4.2630],\n",
       "          [2.1192],\n",
       "          [3.8353],\n",
       "          [4.0135],\n",
       "          [1.5725],\n",
       "          [4.2007],\n",
       "          [2.8974],\n",
       "          [3.0966],\n",
       "          [3.9417]])}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('data/test.csv')\n",
    "dataloader = TextDataLoader(\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=config['MAX_LEN'],\n",
    "    test_data=test,\n",
    "    truncation=True,\n",
    "    batch_size=config['BATCH_SIZE']\n",
    ")\n",
    "    \n",
    "trainer = Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1\n",
    ")\n",
    "\n",
    "trainer.predict(model, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
