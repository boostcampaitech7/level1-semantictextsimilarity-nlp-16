{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/STS/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /data/ephemeral/home/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /data/ephemeral/home/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /data/ephemeral/home/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wandb\n",
    "import yaml\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from utils.tokenizer import get_tokenizer\n",
    "from data_loader.data_loaders import TextDataLoader\n",
    "from utils.util import set_seed\n",
    "from model.model import STSModel\n",
    "from utils.util import WandbCheckpointCallback\n",
    "from utils.clean import clean_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_1</th>\n",
       "      <th>sentence_2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>스릴도있고 반전도 있고 여느 한국영화 쓰레기들하고는 차원이 다르네요~</td>\n",
       "      <td>반전도 있고,사랑도 있고재미도있네요.</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>앗 제가 접근권한이 없다고 뜹니다;;</td>\n",
       "      <td>오, 액세스 권한이 없다고 합니다.</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>주택청약조건 변경해주세요.</td>\n",
       "      <td>주택청약 무주택기준 변경해주세요.</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>입사후 처음 대면으로 만나 반가웠습니다.</td>\n",
       "      <td>화상으로만 보다가 리얼로 만나니 정말 반가웠습니다.</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>뿌듯뿌듯 하네요!!</td>\n",
       "      <td>꼬옥 실제로 한번 뵈어요 뿌뿌뿌~!~!</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               sentence_1                    sentence_2  label\n",
       "0  스릴도있고 반전도 있고 여느 한국영화 쓰레기들하고는 차원이 다르네요~          반전도 있고,사랑도 있고재미도있네요.    2.2\n",
       "1                    앗 제가 접근권한이 없다고 뜹니다;;           오, 액세스 권한이 없다고 합니다.    4.2\n",
       "2                          주택청약조건 변경해주세요.            주택청약 무주택기준 변경해주세요.    2.4\n",
       "3                  입사후 처음 대면으로 만나 반가웠습니다.  화상으로만 보다가 리얼로 만나니 정말 반가웠습니다.    3.0\n",
       "4                              뿌듯뿌듯 하네요!!         꼬옥 실제로 한번 뵈어요 뿌뿌뿌~!~!    0.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "dev = pd.read_csv('data/dev.csv')\n",
    "test = pd.read_csv('data/test.csv')\n",
    "\n",
    "train.drop(columns=['id', 'source', 'binary-label'], inplace = True)\n",
    "dev.drop(columns=['id', 'source', 'binary-label'], inplace = True)\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_1</th>\n",
       "      <th>sentence_2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>스릴도있고 반전도 있고 여느 한국영화 쓰레기들하고는 차원이 다르네요</td>\n",
       "      <td>반전도 있고사랑도 있고재미도있네요</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>앗 제가 접근권한이 없다고 뜹니다</td>\n",
       "      <td>오 액세스 권한이 없다고 합니다</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>주택청약조건 변경해주세요</td>\n",
       "      <td>주택청약 무주택기준 변경해주세요</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>입사후 처음 대면으로 만나 반가웠습니다</td>\n",
       "      <td>화상으로만 보다가 리얼로 만나니 정말 반가웠습니다</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>뿌듯뿌듯 하네요</td>\n",
       "      <td>꼬옥 실제로 한번 뵈어요 뿌뿌뿌</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              sentence_1                   sentence_2  label\n",
       "0  스릴도있고 반전도 있고 여느 한국영화 쓰레기들하고는 차원이 다르네요           반전도 있고사랑도 있고재미도있네요    2.2\n",
       "1                     앗 제가 접근권한이 없다고 뜹니다            오 액세스 권한이 없다고 합니다    4.2\n",
       "2                          주택청약조건 변경해주세요            주택청약 무주택기준 변경해주세요    2.4\n",
       "3                  입사후 처음 대면으로 만나 반가웠습니다  화상으로만 보다가 리얼로 만나니 정말 반가웠습니다    3.0\n",
       "4                               뿌듯뿌듯 하네요            꼬옥 실제로 한번 뵈어요 뿌뿌뿌    0.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[['label']] = train[['label']].astype('float32')\n",
    "train['sentence_1'] = clean_texts(train['sentence_1'])\n",
    "train['sentence_2'] = clean_texts(train['sentence_2'])\n",
    "\n",
    "dev[['label']] = dev[['label']].astype('float32')\n",
    "dev['sentence_1'] = clean_texts(dev['sentence_1'])\n",
    "dev['sentence_2'] = clean_texts(dev['sentence_2'])\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentataion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RS, RD, RI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "import random\n",
    "\n",
    "def random_deletion(text, p=0.1):\n",
    "    mecab = Mecab()\n",
    "    tokens_with_pos = mecab.pos(text)\n",
    "    tokens = [token for token, pos in tokens_with_pos]\n",
    "    if len(tokens) == 1:\n",
    "        return text\n",
    "    remaining = [t for t in tokens if random.random() > p]\n",
    "    if len(remaining) == 0:\n",
    "        return random.choice(tokens)\n",
    "    return ' '.join(remaining)\n",
    "\n",
    "def random_swap(text, n=1):\n",
    "    mecab = Mecab()\n",
    "    tokens_with_pos = mecab.pos(text)\n",
    "    tokens = [token for token, pos in tokens_with_pos]\n",
    "    for _ in range(n):\n",
    "        if len(tokens) >= 2:\n",
    "            idx1, idx2 = random.sample(range(len(tokens)), 2)\n",
    "            tokens[idx1], tokens[idx2] = tokens[idx2], tokens[idx1]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def random_insertion(text, n=1):\n",
    "    mecab = Mecab()\n",
    "    tokens_with_pos = mecab.pos(text)\n",
    "    tokens = [token for token, pos in tokens_with_pos]\n",
    "    for _ in range(n):\n",
    "        insert_pos = random.randint(0, len(tokens))\n",
    "        insert_word = random.choice(tokens)\n",
    "        tokens.insert(insert_pos, insert_word)\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "train_swapped = train.copy(deep=True)\n",
    "train_swapped['sentence_1'] = train_swapped['sentence_1'].apply(lambda x : random_swap(x))\n",
    "train_deleted = train.copy(deep=True)\n",
    "train_deleted['sentence_1'] = train_deleted['sentence_1'].apply(lambda x : random_deletion(x))\n",
    "train_inserted = train.copy(deep=True)\n",
    "train_inserted['sentence_1'] = train_inserted['sentence_1'].apply(lambda x : random_insertion(x))\n",
    "\n",
    "# train = pd.concat([train, train_swapped, train_deleted, train_inserted])\n",
    "train = pd.concat([train, train_inserted])\n",
    "train.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation With Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.iloc[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/STS/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 0/1000....(0.10% done)\n",
      "Index: 1/1000....(0.20% done)\n",
      "Index: 2/1000....(0.30% done)\n",
      "Index: 3/1000....(0.40% done)\n",
      "Index: 4/1000....(0.50% done)\n",
      "Index: 5/1000....(0.60% done)\n",
      "Index: 6/1000....(0.70% done)\n",
      "Index: 7/1000....(0.80% done)\n",
      "Index: 8/1000....(0.90% done)\n",
      "Index: 9/1000....(1.00% done)\n",
      "Index: 10/1000....(1.10% done)\n",
      "Index: 11/1000....(1.20% done)\n",
      "Index: 12/1000....(1.30% done)\n",
      "Index: 13/1000....(1.40% done)\n",
      "Index: 14/1000....(1.50% done)\n",
      "Index: 15/1000....(1.60% done)\n",
      "Index: 16/1000....(1.70% done)\n",
      "Index: 17/1000....(1.80% done)\n",
      "Index: 18/1000....(1.90% done)\n",
      "Index: 19/1000....(2.00% done)\n",
      "Index: 20/1000....(2.10% done)\n",
      "Index: 21/1000....(2.20% done)\n",
      "Index: 22/1000....(2.30% done)\n",
      "Index: 23/1000....(2.40% done)\n",
      "Index: 24/1000....(2.50% done)\n",
      "Index: 25/1000....(2.60% done)\n",
      "Index: 26/1000....(2.70% done)\n",
      "Index: 27/1000....(2.80% done)\n",
      "Index: 28/1000....(2.90% done)\n",
      "Index: 29/1000....(3.00% done)\n",
      "Index: 30/1000....(3.10% done)\n",
      "Index: 31/1000....(3.20% done)\n",
      "Index: 32/1000....(3.30% done)\n",
      "Index: 33/1000....(3.40% done)\n",
      "Index: 34/1000....(3.50% done)\n",
      "Index: 35/1000....(3.60% done)\n",
      "Index: 36/1000....(3.70% done)\n",
      "Index: 37/1000....(3.80% done)\n",
      "Index: 38/1000....(3.90% done)\n",
      "Index: 39/1000....(4.00% done)\n",
      "Index: 40/1000....(4.10% done)\n",
      "Index: 41/1000....(4.20% done)\n",
      "Index: 42/1000....(4.30% done)\n",
      "Index: 43/1000....(4.40% done)\n",
      "Index: 44/1000....(4.50% done)\n",
      "Index: 45/1000....(4.60% done)\n",
      "Index: 46/1000....(4.70% done)\n",
      "Index: 47/1000....(4.80% done)\n",
      "Index: 48/1000....(4.90% done)\n",
      "Index: 49/1000....(5.00% done)\n",
      "Index: 50/1000....(5.10% done)\n",
      "Index: 51/1000....(5.20% done)\n",
      "Index: 52/1000....(5.30% done)\n",
      "Index: 53/1000....(5.40% done)\n",
      "Index: 54/1000....(5.50% done)\n",
      "Index: 55/1000....(5.60% done)\n",
      "Index: 56/1000....(5.70% done)\n",
      "Index: 57/1000....(5.80% done)\n",
      "Index: 58/1000....(5.90% done)\n",
      "Index: 59/1000....(6.00% done)\n",
      "Index: 60/1000....(6.10% done)\n",
      "Index: 61/1000....(6.20% done)\n",
      "Index: 62/1000....(6.30% done)\n",
      "Index: 63/1000....(6.40% done)\n",
      "Index: 64/1000....(6.50% done)\n",
      "Index: 65/1000....(6.60% done)\n",
      "Index: 66/1000....(6.70% done)\n",
      "Index: 67/1000....(6.80% done)\n",
      "Index: 68/1000....(6.90% done)\n",
      "Index: 69/1000....(7.00% done)\n",
      "Index: 70/1000....(7.10% done)\n",
      "Index: 71/1000....(7.20% done)\n",
      "Index: 72/1000....(7.30% done)\n",
      "Index: 73/1000....(7.40% done)\n",
      "Index: 74/1000....(7.50% done)\n",
      "Index: 75/1000....(7.60% done)\n",
      "Index: 76/1000....(7.70% done)\n",
      "Index: 77/1000....(7.80% done)\n",
      "Index: 78/1000....(7.90% done)\n",
      "Index: 79/1000....(8.00% done)\n",
      "Index: 80/1000....(8.10% done)\n",
      "Index: 81/1000....(8.20% done)\n",
      "Index: 82/1000....(8.30% done)\n",
      "Index: 83/1000....(8.40% done)\n",
      "Index: 84/1000....(8.50% done)\n",
      "Index: 85/1000....(8.60% done)\n",
      "Index: 86/1000....(8.70% done)\n",
      "Index: 87/1000....(8.80% done)\n",
      "Index: 88/1000....(8.90% done)\n",
      "Index: 89/1000....(9.00% done)\n",
      "Index: 90/1000....(9.10% done)\n",
      "Index: 91/1000....(9.20% done)\n",
      "Index: 92/1000....(9.30% done)\n",
      "Index: 93/1000....(9.40% done)\n",
      "Index: 94/1000....(9.50% done)\n",
      "Index: 95/1000....(9.60% done)\n",
      "Index: 96/1000....(9.70% done)\n",
      "Index: 97/1000....(9.80% done)\n",
      "Index: 98/1000....(9.90% done)\n",
      "Index: 99/1000....(10.00% done)\n",
      "Index: 100/1000....(10.10% done)\n",
      "Index: 101/1000....(10.20% done)\n",
      "Index: 102/1000....(10.30% done)\n",
      "Index: 103/1000....(10.40% done)\n",
      "Index: 104/1000....(10.50% done)\n",
      "Index: 105/1000....(10.60% done)\n",
      "Index: 106/1000....(10.70% done)\n",
      "Index: 107/1000....(10.80% done)\n",
      "Index: 108/1000....(10.90% done)\n",
      "Index: 109/1000....(11.00% done)\n",
      "Index: 110/1000....(11.10% done)\n",
      "Index: 111/1000....(11.20% done)\n",
      "Index: 112/1000....(11.30% done)\n",
      "Index: 113/1000....(11.40% done)\n",
      "Index: 114/1000....(11.50% done)\n",
      "Index: 115/1000....(11.60% done)\n",
      "Index: 116/1000....(11.70% done)\n",
      "Index: 117/1000....(11.80% done)\n",
      "Index: 118/1000....(11.90% done)\n",
      "Index: 119/1000....(12.00% done)\n",
      "Index: 120/1000....(12.10% done)\n",
      "Index: 121/1000....(12.20% done)\n",
      "Index: 122/1000....(12.30% done)\n",
      "Index: 123/1000....(12.40% done)\n",
      "Index: 124/1000....(12.50% done)\n",
      "Index: 125/1000....(12.60% done)\n",
      "Index: 126/1000....(12.70% done)\n",
      "Index: 127/1000....(12.80% done)\n",
      "Index: 128/1000....(12.90% done)\n",
      "Index: 129/1000....(13.00% done)\n",
      "Index: 130/1000....(13.10% done)\n",
      "Index: 131/1000....(13.20% done)\n",
      "Index: 132/1000....(13.30% done)\n",
      "Index: 133/1000....(13.40% done)\n",
      "Index: 134/1000....(13.50% done)\n",
      "Index: 135/1000....(13.60% done)\n",
      "Index: 136/1000....(13.70% done)\n",
      "Index: 137/1000....(13.80% done)\n",
      "Index: 138/1000....(13.90% done)\n",
      "Index: 139/1000....(14.00% done)\n",
      "Index: 140/1000....(14.10% done)\n",
      "Index: 141/1000....(14.20% done)\n",
      "Index: 142/1000....(14.30% done)\n",
      "Index: 143/1000....(14.40% done)\n",
      "Index: 144/1000....(14.50% done)\n",
      "Index: 145/1000....(14.60% done)\n",
      "Index: 146/1000....(14.70% done)\n",
      "Index: 147/1000....(14.80% done)\n",
      "Index: 148/1000....(14.90% done)\n",
      "Index: 149/1000....(15.00% done)\n",
      "Index: 150/1000....(15.10% done)\n",
      "Index: 151/1000....(15.20% done)\n",
      "Index: 152/1000....(15.30% done)\n",
      "Index: 153/1000....(15.40% done)\n",
      "Index: 154/1000....(15.50% done)\n",
      "Index: 155/1000....(15.60% done)\n",
      "Index: 156/1000....(15.70% done)\n",
      "Index: 157/1000....(15.80% done)\n",
      "Index: 158/1000....(15.90% done)\n",
      "Index: 159/1000....(16.00% done)\n",
      "Index: 160/1000....(16.10% done)\n",
      "Index: 161/1000....(16.20% done)\n",
      "Index: 162/1000....(16.30% done)\n",
      "Index: 163/1000....(16.40% done)\n",
      "Index: 164/1000....(16.50% done)\n",
      "Index: 165/1000....(16.60% done)\n",
      "Index: 166/1000....(16.70% done)\n",
      "Index: 167/1000....(16.80% done)\n",
      "Index: 168/1000....(16.90% done)\n",
      "Index: 169/1000....(17.00% done)\n",
      "Index: 170/1000....(17.10% done)\n",
      "Index: 171/1000....(17.20% done)\n",
      "Index: 172/1000....(17.30% done)\n",
      "Index: 173/1000....(17.40% done)\n",
      "Index: 174/1000....(17.50% done)\n",
      "Index: 175/1000....(17.60% done)\n",
      "Index: 176/1000....(17.70% done)\n",
      "Index: 177/1000....(17.80% done)\n",
      "Index: 178/1000....(17.90% done)\n",
      "Index: 179/1000....(18.00% done)\n",
      "Index: 180/1000....(18.10% done)\n",
      "Index: 181/1000....(18.20% done)\n",
      "Index: 182/1000....(18.30% done)\n",
      "Index: 183/1000....(18.40% done)\n",
      "Index: 184/1000....(18.50% done)\n",
      "Index: 185/1000....(18.60% done)\n",
      "Index: 186/1000....(18.70% done)\n",
      "Index: 187/1000....(18.80% done)\n",
      "Index: 188/1000....(18.90% done)\n",
      "Index: 189/1000....(19.00% done)\n",
      "Index: 190/1000....(19.10% done)\n",
      "Index: 191/1000....(19.20% done)\n",
      "Index: 192/1000....(19.30% done)\n",
      "Index: 193/1000....(19.40% done)\n",
      "Index: 194/1000....(19.50% done)\n",
      "Index: 195/1000....(19.60% done)\n",
      "Index: 196/1000....(19.70% done)\n",
      "Index: 197/1000....(19.80% done)\n",
      "Index: 198/1000....(19.90% done)\n",
      "Index: 199/1000....(20.00% done)\n",
      "Index: 200/1000....(20.10% done)\n",
      "Index: 201/1000....(20.20% done)\n",
      "Index: 202/1000....(20.30% done)\n",
      "Index: 203/1000....(20.40% done)\n",
      "Index: 204/1000....(20.50% done)\n",
      "Index: 205/1000....(20.60% done)\n",
      "Index: 206/1000....(20.70% done)\n",
      "Index: 207/1000....(20.80% done)\n",
      "Index: 208/1000....(20.90% done)\n",
      "Index: 209/1000....(21.00% done)\n",
      "Index: 210/1000....(21.10% done)\n",
      "Index: 211/1000....(21.20% done)\n",
      "Index: 212/1000....(21.30% done)\n",
      "Index: 213/1000....(21.40% done)\n",
      "Index: 214/1000....(21.50% done)\n",
      "Index: 215/1000....(21.60% done)\n",
      "Index: 216/1000....(21.70% done)\n",
      "Index: 217/1000....(21.80% done)\n",
      "Index: 218/1000....(21.90% done)\n",
      "Index: 219/1000....(22.00% done)\n",
      "Index: 220/1000....(22.10% done)\n",
      "Index: 221/1000....(22.20% done)\n",
      "Index: 222/1000....(22.30% done)\n",
      "Index: 223/1000....(22.40% done)\n",
      "Index: 224/1000....(22.50% done)\n",
      "Index: 225/1000....(22.60% done)\n",
      "Index: 226/1000....(22.70% done)\n",
      "Index: 227/1000....(22.80% done)\n",
      "Index: 228/1000....(22.90% done)\n",
      "Index: 229/1000....(23.00% done)\n",
      "Index: 230/1000....(23.10% done)\n",
      "Index: 231/1000....(23.20% done)\n",
      "Index: 232/1000....(23.30% done)\n",
      "Index: 233/1000....(23.40% done)\n",
      "Index: 234/1000....(23.50% done)\n",
      "Index: 235/1000....(23.60% done)\n",
      "Index: 236/1000....(23.70% done)\n",
      "Index: 237/1000....(23.80% done)\n",
      "Index: 238/1000....(23.90% done)\n",
      "Index: 239/1000....(24.00% done)\n",
      "Index: 240/1000....(24.10% done)\n",
      "Index: 241/1000....(24.20% done)\n",
      "Index: 242/1000....(24.30% done)\n",
      "Index: 243/1000....(24.40% done)\n",
      "Index: 244/1000....(24.50% done)\n",
      "Index: 245/1000....(24.60% done)\n",
      "Index: 246/1000....(24.70% done)\n",
      "Index: 247/1000....(24.80% done)\n",
      "Index: 248/1000....(24.90% done)\n",
      "Index: 249/1000....(25.00% done)\n",
      "Index: 250/1000....(25.10% done)\n",
      "Index: 251/1000....(25.20% done)\n",
      "Index: 252/1000....(25.30% done)\n",
      "Index: 253/1000....(25.40% done)\n",
      "Index: 254/1000....(25.50% done)\n",
      "Index: 255/1000....(25.60% done)\n",
      "Index: 256/1000....(25.70% done)\n",
      "Index: 257/1000....(25.80% done)\n",
      "Index: 258/1000....(25.90% done)\n",
      "Index: 259/1000....(26.00% done)\n",
      "Index: 260/1000....(26.10% done)\n",
      "Index: 261/1000....(26.20% done)\n",
      "Index: 262/1000....(26.30% done)\n",
      "Index: 263/1000....(26.40% done)\n",
      "Index: 264/1000....(26.50% done)\n",
      "Index: 265/1000....(26.60% done)\n",
      "Index: 266/1000....(26.70% done)\n",
      "Index: 267/1000....(26.80% done)\n",
      "Index: 268/1000....(26.90% done)\n",
      "Index: 269/1000....(27.00% done)\n",
      "Index: 270/1000....(27.10% done)\n",
      "Index: 271/1000....(27.20% done)\n",
      "Index: 272/1000....(27.30% done)\n",
      "Index: 273/1000....(27.40% done)\n",
      "Index: 274/1000....(27.50% done)\n",
      "Index: 275/1000....(27.60% done)\n",
      "Index: 276/1000....(27.70% done)\n",
      "Index: 277/1000....(27.80% done)\n",
      "Index: 278/1000....(27.90% done)\n",
      "Index: 279/1000....(28.00% done)\n",
      "Index: 280/1000....(28.10% done)\n",
      "Index: 281/1000....(28.20% done)\n",
      "Index: 282/1000....(28.30% done)\n",
      "Index: 283/1000....(28.40% done)\n",
      "Index: 284/1000....(28.50% done)\n",
      "Index: 285/1000....(28.60% done)\n",
      "Index: 286/1000....(28.70% done)\n",
      "Index: 287/1000....(28.80% done)\n",
      "Index: 288/1000....(28.90% done)\n",
      "Index: 289/1000....(29.00% done)\n",
      "Index: 290/1000....(29.10% done)\n",
      "Index: 291/1000....(29.20% done)\n",
      "Index: 292/1000....(29.30% done)\n",
      "Index: 293/1000....(29.40% done)\n",
      "Index: 294/1000....(29.50% done)\n",
      "Index: 295/1000....(29.60% done)\n",
      "Index: 296/1000....(29.70% done)\n",
      "Index: 297/1000....(29.80% done)\n",
      "Index: 298/1000....(29.90% done)\n",
      "Index: 299/1000....(30.00% done)\n",
      "Index: 300/1000....(30.10% done)\n",
      "Index: 301/1000....(30.20% done)\n",
      "Index: 302/1000....(30.30% done)\n",
      "Index: 303/1000....(30.40% done)\n",
      "Index: 304/1000....(30.50% done)\n",
      "Index: 305/1000....(30.60% done)\n",
      "Index: 306/1000....(30.70% done)\n",
      "Index: 307/1000....(30.80% done)\n",
      "Index: 308/1000....(30.90% done)\n",
      "Index: 309/1000....(31.00% done)\n",
      "Index: 310/1000....(31.10% done)\n",
      "Index: 311/1000....(31.20% done)\n",
      "Index: 312/1000....(31.30% done)\n",
      "Index: 313/1000....(31.40% done)\n",
      "Index: 314/1000....(31.50% done)\n",
      "Index: 315/1000....(31.60% done)\n",
      "Index: 316/1000....(31.70% done)\n",
      "Index: 317/1000....(31.80% done)\n",
      "Index: 318/1000....(31.90% done)\n",
      "Index: 319/1000....(32.00% done)\n",
      "Index: 320/1000....(32.10% done)\n",
      "Index: 321/1000....(32.20% done)\n",
      "Index: 322/1000....(32.30% done)\n",
      "Index: 323/1000....(32.40% done)\n",
      "Index: 324/1000....(32.50% done)\n",
      "Index: 325/1000....(32.60% done)\n",
      "Index: 326/1000....(32.70% done)\n",
      "Index: 327/1000....(32.80% done)\n",
      "Index: 328/1000....(32.90% done)\n",
      "Index: 329/1000....(33.00% done)\n",
      "Index: 330/1000....(33.10% done)\n",
      "Index: 331/1000....(33.20% done)\n",
      "Index: 332/1000....(33.30% done)\n",
      "Index: 333/1000....(33.40% done)\n",
      "Index: 334/1000....(33.50% done)\n",
      "Index: 335/1000....(33.60% done)\n",
      "Index: 336/1000....(33.70% done)\n",
      "Index: 337/1000....(33.80% done)\n",
      "Index: 338/1000....(33.90% done)\n",
      "Index: 339/1000....(34.00% done)\n",
      "Index: 340/1000....(34.10% done)\n",
      "Index: 341/1000....(34.20% done)\n",
      "Index: 342/1000....(34.30% done)\n",
      "Index: 343/1000....(34.40% done)\n",
      "Index: 344/1000....(34.50% done)\n",
      "Index: 345/1000....(34.60% done)\n",
      "Index: 346/1000....(34.70% done)\n",
      "Index: 347/1000....(34.80% done)\n",
      "Index: 348/1000....(34.90% done)\n",
      "Index: 349/1000....(35.00% done)\n",
      "Index: 350/1000....(35.10% done)\n",
      "Index: 351/1000....(35.20% done)\n",
      "Index: 352/1000....(35.30% done)\n",
      "Index: 353/1000....(35.40% done)\n",
      "Index: 354/1000....(35.50% done)\n",
      "Index: 355/1000....(35.60% done)\n",
      "Index: 356/1000....(35.70% done)\n",
      "Index: 357/1000....(35.80% done)\n",
      "Index: 358/1000....(35.90% done)\n",
      "Index: 359/1000....(36.00% done)\n",
      "Index: 360/1000....(36.10% done)\n",
      "Index: 361/1000....(36.20% done)\n",
      "Index: 362/1000....(36.30% done)\n",
      "Index: 363/1000....(36.40% done)\n",
      "Index: 364/1000....(36.50% done)\n",
      "Index: 365/1000....(36.60% done)\n",
      "Index: 366/1000....(36.70% done)\n",
      "Index: 367/1000....(36.80% done)\n",
      "Index: 368/1000....(36.90% done)\n",
      "Index: 369/1000....(37.00% done)\n",
      "Index: 370/1000....(37.10% done)\n",
      "Index: 371/1000....(37.20% done)\n",
      "Index: 372/1000....(37.30% done)\n",
      "Index: 373/1000....(37.40% done)\n",
      "Index: 374/1000....(37.50% done)\n",
      "Index: 375/1000....(37.60% done)\n",
      "Index: 376/1000....(37.70% done)\n",
      "Index: 377/1000....(37.80% done)\n",
      "Index: 378/1000....(37.90% done)\n",
      "Index: 379/1000....(38.00% done)\n",
      "Index: 380/1000....(38.10% done)\n",
      "Index: 381/1000....(38.20% done)\n",
      "Index: 382/1000....(38.30% done)\n",
      "Index: 383/1000....(38.40% done)\n",
      "Index: 384/1000....(38.50% done)\n",
      "Index: 385/1000....(38.60% done)\n",
      "Index: 386/1000....(38.70% done)\n",
      "Index: 387/1000....(38.80% done)\n",
      "Index: 388/1000....(38.90% done)\n",
      "Index: 389/1000....(39.00% done)\n",
      "Index: 390/1000....(39.10% done)\n",
      "Index: 391/1000....(39.20% done)\n",
      "Index: 392/1000....(39.30% done)\n",
      "Index: 393/1000....(39.40% done)\n",
      "Index: 394/1000....(39.50% done)\n",
      "Index: 395/1000....(39.60% done)\n",
      "Index: 396/1000....(39.70% done)\n",
      "Index: 397/1000....(39.80% done)\n",
      "Index: 398/1000....(39.90% done)\n",
      "Index: 399/1000....(40.00% done)\n",
      "Index: 400/1000....(40.10% done)\n",
      "Index: 401/1000....(40.20% done)\n",
      "Index: 402/1000....(40.30% done)\n",
      "Index: 403/1000....(40.40% done)\n",
      "Index: 404/1000....(40.50% done)\n",
      "Index: 405/1000....(40.60% done)\n",
      "Index: 406/1000....(40.70% done)\n",
      "Index: 407/1000....(40.80% done)\n",
      "Index: 408/1000....(40.90% done)\n",
      "Index: 409/1000....(41.00% done)\n",
      "Index: 410/1000....(41.10% done)\n",
      "Index: 411/1000....(41.20% done)\n",
      "Index: 412/1000....(41.30% done)\n",
      "Index: 413/1000....(41.40% done)\n",
      "Index: 414/1000....(41.50% done)\n",
      "Index: 415/1000....(41.60% done)\n",
      "Index: 416/1000....(41.70% done)\n",
      "Index: 417/1000....(41.80% done)\n",
      "Index: 418/1000....(41.90% done)\n",
      "Index: 419/1000....(42.00% done)\n",
      "Index: 420/1000....(42.10% done)\n",
      "Index: 421/1000....(42.20% done)\n",
      "Index: 422/1000....(42.30% done)\n",
      "Index: 423/1000....(42.40% done)\n",
      "Index: 424/1000....(42.50% done)\n",
      "Index: 425/1000....(42.60% done)\n",
      "Index: 426/1000....(42.70% done)\n",
      "Index: 427/1000....(42.80% done)\n",
      "Index: 428/1000....(42.90% done)\n",
      "Index: 429/1000....(43.00% done)\n",
      "Index: 430/1000....(43.10% done)\n",
      "Index: 431/1000....(43.20% done)\n",
      "Index: 432/1000....(43.30% done)\n",
      "Index: 433/1000....(43.40% done)\n",
      "Index: 434/1000....(43.50% done)\n",
      "Index: 435/1000....(43.60% done)\n",
      "Index: 436/1000....(43.70% done)\n",
      "Index: 437/1000....(43.80% done)\n",
      "Index: 438/1000....(43.90% done)\n",
      "Index: 439/1000....(44.00% done)\n",
      "Index: 440/1000....(44.10% done)\n",
      "Index: 441/1000....(44.20% done)\n",
      "Index: 442/1000....(44.30% done)\n",
      "Index: 443/1000....(44.40% done)\n",
      "Index: 444/1000....(44.50% done)\n",
      "Index: 445/1000....(44.60% done)\n",
      "Index: 446/1000....(44.70% done)\n",
      "Index: 447/1000....(44.80% done)\n",
      "Index: 448/1000....(44.90% done)\n",
      "Index: 449/1000....(45.00% done)\n",
      "Index: 450/1000....(45.10% done)\n",
      "Index: 451/1000....(45.20% done)\n",
      "Index: 452/1000....(45.30% done)\n",
      "Index: 453/1000....(45.40% done)\n",
      "Index: 454/1000....(45.50% done)\n",
      "Index: 455/1000....(45.60% done)\n",
      "Index: 456/1000....(45.70% done)\n",
      "Index: 457/1000....(45.80% done)\n",
      "Index: 458/1000....(45.90% done)\n",
      "Index: 459/1000....(46.00% done)\n",
      "Index: 460/1000....(46.10% done)\n",
      "Index: 461/1000....(46.20% done)\n",
      "Index: 462/1000....(46.30% done)\n",
      "Index: 463/1000....(46.40% done)\n",
      "Index: 464/1000....(46.50% done)\n",
      "Index: 465/1000....(46.60% done)\n",
      "Index: 466/1000....(46.70% done)\n",
      "Index: 467/1000....(46.80% done)\n",
      "Index: 468/1000....(46.90% done)\n",
      "Index: 469/1000....(47.00% done)\n",
      "Index: 470/1000....(47.10% done)\n",
      "Index: 471/1000....(47.20% done)\n",
      "Index: 472/1000....(47.30% done)\n",
      "Index: 473/1000....(47.40% done)\n",
      "Index: 474/1000....(47.50% done)\n",
      "Index: 475/1000....(47.60% done)\n",
      "Index: 476/1000....(47.70% done)\n",
      "Index: 477/1000....(47.80% done)\n",
      "Index: 478/1000....(47.90% done)\n",
      "Index: 479/1000....(48.00% done)\n",
      "Index: 480/1000....(48.10% done)\n",
      "Index: 481/1000....(48.20% done)\n",
      "Index: 482/1000....(48.30% done)\n",
      "Index: 483/1000....(48.40% done)\n",
      "Index: 484/1000....(48.50% done)\n",
      "Index: 485/1000....(48.60% done)\n",
      "Index: 486/1000....(48.70% done)\n",
      "Index: 487/1000....(48.80% done)\n",
      "Index: 488/1000....(48.90% done)\n",
      "Index: 489/1000....(49.00% done)\n",
      "Index: 490/1000....(49.10% done)\n",
      "Index: 491/1000....(49.20% done)\n",
      "Index: 492/1000....(49.30% done)\n",
      "Index: 493/1000....(49.40% done)\n",
      "Index: 494/1000....(49.50% done)\n",
      "Index: 495/1000....(49.60% done)\n",
      "Index: 496/1000....(49.70% done)\n",
      "Index: 497/1000....(49.80% done)\n",
      "Index: 498/1000....(49.90% done)\n",
      "Index: 499/1000....(50.00% done)\n",
      "Index: 500/1000....(50.10% done)\n",
      "Index: 501/1000....(50.20% done)\n",
      "Index: 502/1000....(50.30% done)\n",
      "Index: 503/1000....(50.40% done)\n",
      "Index: 504/1000....(50.50% done)\n",
      "Index: 505/1000....(50.60% done)\n",
      "Index: 506/1000....(50.70% done)\n",
      "Index: 507/1000....(50.80% done)\n",
      "Index: 508/1000....(50.90% done)\n",
      "Index: 509/1000....(51.00% done)\n",
      "Index: 510/1000....(51.10% done)\n",
      "Index: 511/1000....(51.20% done)\n",
      "Index: 512/1000....(51.30% done)\n",
      "Index: 513/1000....(51.40% done)\n",
      "Index: 514/1000....(51.50% done)\n",
      "Index: 515/1000....(51.60% done)\n",
      "Index: 516/1000....(51.70% done)\n",
      "Index: 517/1000....(51.80% done)\n",
      "Index: 518/1000....(51.90% done)\n",
      "Index: 519/1000....(52.00% done)\n",
      "Index: 520/1000....(52.10% done)\n",
      "Index: 521/1000....(52.20% done)\n",
      "Index: 522/1000....(52.30% done)\n",
      "Index: 523/1000....(52.40% done)\n",
      "Index: 524/1000....(52.50% done)\n",
      "Index: 525/1000....(52.60% done)\n",
      "Index: 526/1000....(52.70% done)\n",
      "Index: 527/1000....(52.80% done)\n",
      "Index: 528/1000....(52.90% done)\n",
      "Index: 529/1000....(53.00% done)\n",
      "Index: 530/1000....(53.10% done)\n",
      "Index: 531/1000....(53.20% done)\n",
      "Index: 532/1000....(53.30% done)\n",
      "Index: 533/1000....(53.40% done)\n",
      "Index: 534/1000....(53.50% done)\n",
      "Index: 535/1000....(53.60% done)\n",
      "Index: 536/1000....(53.70% done)\n",
      "Index: 537/1000....(53.80% done)\n",
      "Index: 538/1000....(53.90% done)\n",
      "Index: 539/1000....(54.00% done)\n",
      "Index: 540/1000....(54.10% done)\n",
      "Index: 541/1000....(54.20% done)\n",
      "Index: 542/1000....(54.30% done)\n",
      "Index: 543/1000....(54.40% done)\n",
      "Index: 544/1000....(54.50% done)\n",
      "Index: 545/1000....(54.60% done)\n",
      "Index: 546/1000....(54.70% done)\n",
      "Index: 547/1000....(54.80% done)\n",
      "Index: 548/1000....(54.90% done)\n",
      "Index: 549/1000....(55.00% done)\n",
      "Index: 550/1000....(55.10% done)\n",
      "Index: 551/1000....(55.20% done)\n",
      "Index: 552/1000....(55.30% done)\n",
      "Index: 553/1000....(55.40% done)\n",
      "Index: 554/1000....(55.50% done)\n",
      "Index: 555/1000....(55.60% done)\n",
      "Index: 556/1000....(55.70% done)\n",
      "Index: 557/1000....(55.80% done)\n",
      "Index: 558/1000....(55.90% done)\n",
      "Index: 559/1000....(56.00% done)\n",
      "Index: 560/1000....(56.10% done)\n",
      "Index: 561/1000....(56.20% done)\n",
      "Index: 562/1000....(56.30% done)\n",
      "Index: 563/1000....(56.40% done)\n",
      "Index: 564/1000....(56.50% done)\n",
      "Index: 565/1000....(56.60% done)\n",
      "Index: 566/1000....(56.70% done)\n",
      "Index: 567/1000....(56.80% done)\n",
      "Index: 568/1000....(56.90% done)\n",
      "Index: 569/1000....(57.00% done)\n",
      "Index: 570/1000....(57.10% done)\n",
      "Index: 571/1000....(57.20% done)\n",
      "Index: 572/1000....(57.30% done)\n",
      "Index: 573/1000....(57.40% done)\n",
      "Index: 574/1000....(57.50% done)\n",
      "Index: 575/1000....(57.60% done)\n",
      "Index: 576/1000....(57.70% done)\n",
      "Index: 577/1000....(57.80% done)\n",
      "Index: 578/1000....(57.90% done)\n",
      "Index: 579/1000....(58.00% done)\n",
      "Index: 580/1000....(58.10% done)\n",
      "Index: 581/1000....(58.20% done)\n",
      "Index: 582/1000....(58.30% done)\n",
      "Index: 583/1000....(58.40% done)\n",
      "Index: 584/1000....(58.50% done)\n",
      "Index: 585/1000....(58.60% done)\n",
      "Index: 586/1000....(58.70% done)\n",
      "Index: 587/1000....(58.80% done)\n",
      "Index: 588/1000....(58.90% done)\n",
      "Index: 589/1000....(59.00% done)\n",
      "Index: 590/1000....(59.10% done)\n",
      "Index: 591/1000....(59.20% done)\n",
      "Index: 592/1000....(59.30% done)\n",
      "Index: 593/1000....(59.40% done)\n",
      "Index: 594/1000....(59.50% done)\n",
      "Index: 595/1000....(59.60% done)\n",
      "Index: 596/1000....(59.70% done)\n",
      "Index: 597/1000....(59.80% done)\n",
      "Index: 598/1000....(59.90% done)\n",
      "Index: 599/1000....(60.00% done)\n",
      "Index: 600/1000....(60.10% done)\n",
      "Index: 601/1000....(60.20% done)\n",
      "Index: 602/1000....(60.30% done)\n",
      "Index: 603/1000....(60.40% done)\n",
      "Index: 604/1000....(60.50% done)\n",
      "Index: 605/1000....(60.60% done)\n",
      "Index: 606/1000....(60.70% done)\n",
      "Index: 607/1000....(60.80% done)\n",
      "Index: 608/1000....(60.90% done)\n",
      "Index: 609/1000....(61.00% done)\n",
      "Index: 610/1000....(61.10% done)\n",
      "Index: 611/1000....(61.20% done)\n",
      "Index: 612/1000....(61.30% done)\n",
      "Index: 613/1000....(61.40% done)\n",
      "Index: 614/1000....(61.50% done)\n",
      "Index: 615/1000....(61.60% done)\n",
      "Index: 616/1000....(61.70% done)\n",
      "Index: 617/1000....(61.80% done)\n",
      "Index: 618/1000....(61.90% done)\n",
      "Index: 619/1000....(62.00% done)\n",
      "Index: 620/1000....(62.10% done)\n",
      "Index: 621/1000....(62.20% done)\n",
      "Index: 622/1000....(62.30% done)\n",
      "Index: 623/1000....(62.40% done)\n",
      "Index: 624/1000....(62.50% done)\n",
      "Index: 625/1000....(62.60% done)\n",
      "Index: 626/1000....(62.70% done)\n",
      "Index: 627/1000....(62.80% done)\n",
      "Index: 628/1000....(62.90% done)\n",
      "Index: 629/1000....(63.00% done)\n",
      "Index: 630/1000....(63.10% done)\n",
      "Index: 631/1000....(63.20% done)\n",
      "Index: 632/1000....(63.30% done)\n",
      "Index: 633/1000....(63.40% done)\n",
      "Index: 634/1000....(63.50% done)\n",
      "Index: 635/1000....(63.60% done)\n",
      "Index: 636/1000....(63.70% done)\n",
      "Index: 637/1000....(63.80% done)\n",
      "Index: 638/1000....(63.90% done)\n",
      "Index: 639/1000....(64.00% done)\n",
      "Index: 640/1000....(64.10% done)\n",
      "Index: 641/1000....(64.20% done)\n",
      "Index: 642/1000....(64.30% done)\n",
      "Index: 643/1000....(64.40% done)\n",
      "Index: 644/1000....(64.50% done)\n",
      "Index: 645/1000....(64.60% done)\n",
      "Index: 646/1000....(64.70% done)\n",
      "Index: 647/1000....(64.80% done)\n",
      "Index: 648/1000....(64.90% done)\n",
      "Index: 649/1000....(65.00% done)\n",
      "Index: 650/1000....(65.10% done)\n",
      "Index: 651/1000....(65.20% done)\n",
      "Index: 652/1000....(65.30% done)\n",
      "Index: 653/1000....(65.40% done)\n",
      "Index: 654/1000....(65.50% done)\n",
      "Index: 655/1000....(65.60% done)\n",
      "Index: 656/1000....(65.70% done)\n",
      "Index: 657/1000....(65.80% done)\n",
      "Index: 658/1000....(65.90% done)\n",
      "Index: 659/1000....(66.00% done)\n",
      "Index: 660/1000....(66.10% done)\n",
      "Index: 661/1000....(66.20% done)\n",
      "Index: 662/1000....(66.30% done)\n",
      "Index: 663/1000....(66.40% done)\n",
      "Index: 664/1000....(66.50% done)\n",
      "Index: 665/1000....(66.60% done)\n",
      "Index: 666/1000....(66.70% done)\n",
      "Index: 667/1000....(66.80% done)\n",
      "Index: 668/1000....(66.90% done)\n",
      "Index: 669/1000....(67.00% done)\n",
      "Index: 670/1000....(67.10% done)\n",
      "Index: 671/1000....(67.20% done)\n",
      "Index: 672/1000....(67.30% done)\n",
      "Index: 673/1000....(67.40% done)\n",
      "Index: 674/1000....(67.50% done)\n",
      "Index: 675/1000....(67.60% done)\n",
      "Index: 676/1000....(67.70% done)\n",
      "Index: 677/1000....(67.80% done)\n",
      "Index: 678/1000....(67.90% done)\n",
      "Index: 679/1000....(68.00% done)\n",
      "Index: 680/1000....(68.10% done)\n",
      "Index: 681/1000....(68.20% done)\n",
      "Index: 682/1000....(68.30% done)\n",
      "Index: 683/1000....(68.40% done)\n",
      "Index: 684/1000....(68.50% done)\n",
      "Index: 685/1000....(68.60% done)\n",
      "Index: 686/1000....(68.70% done)\n",
      "Index: 687/1000....(68.80% done)\n",
      "Index: 688/1000....(68.90% done)\n",
      "Index: 689/1000....(69.00% done)\n",
      "Index: 690/1000....(69.10% done)\n",
      "Index: 691/1000....(69.20% done)\n",
      "Index: 692/1000....(69.30% done)\n",
      "Index: 693/1000....(69.40% done)\n",
      "Index: 694/1000....(69.50% done)\n",
      "Index: 695/1000....(69.60% done)\n",
      "Index: 696/1000....(69.70% done)\n",
      "Index: 697/1000....(69.80% done)\n",
      "Index: 698/1000....(69.90% done)\n",
      "Index: 699/1000....(70.00% done)\n",
      "Index: 700/1000....(70.10% done)\n",
      "Index: 701/1000....(70.20% done)\n",
      "Index: 702/1000....(70.30% done)\n",
      "Index: 703/1000....(70.40% done)\n",
      "Index: 704/1000....(70.50% done)\n",
      "Index: 705/1000....(70.60% done)\n",
      "Index: 706/1000....(70.70% done)\n",
      "Index: 707/1000....(70.80% done)\n",
      "Index: 708/1000....(70.90% done)\n",
      "Index: 709/1000....(71.00% done)\n",
      "Index: 710/1000....(71.10% done)\n",
      "Index: 711/1000....(71.20% done)\n",
      "Index: 712/1000....(71.30% done)\n",
      "Index: 713/1000....(71.40% done)\n",
      "Index: 714/1000....(71.50% done)\n",
      "Index: 715/1000....(71.60% done)\n",
      "Index: 716/1000....(71.70% done)\n",
      "Index: 717/1000....(71.80% done)\n",
      "Index: 718/1000....(71.90% done)\n",
      "Index: 719/1000....(72.00% done)\n",
      "Index: 720/1000....(72.10% done)\n",
      "Index: 721/1000....(72.20% done)\n",
      "Index: 722/1000....(72.30% done)\n",
      "Index: 723/1000....(72.40% done)\n",
      "Index: 724/1000....(72.50% done)\n",
      "Index: 725/1000....(72.60% done)\n",
      "Index: 726/1000....(72.70% done)\n",
      "Index: 727/1000....(72.80% done)\n",
      "Index: 728/1000....(72.90% done)\n",
      "Index: 729/1000....(73.00% done)\n",
      "Index: 730/1000....(73.10% done)\n",
      "Index: 731/1000....(73.20% done)\n",
      "Index: 732/1000....(73.30% done)\n",
      "Index: 733/1000....(73.40% done)\n",
      "Index: 734/1000....(73.50% done)\n",
      "Index: 735/1000....(73.60% done)\n",
      "Index: 736/1000....(73.70% done)\n",
      "Index: 737/1000....(73.80% done)\n",
      "Index: 738/1000....(73.90% done)\n",
      "Index: 739/1000....(74.00% done)\n",
      "Index: 740/1000....(74.10% done)\n",
      "Index: 741/1000....(74.20% done)\n",
      "Index: 742/1000....(74.30% done)\n",
      "Index: 743/1000....(74.40% done)\n",
      "Index: 744/1000....(74.50% done)\n",
      "Index: 745/1000....(74.60% done)\n",
      "Index: 746/1000....(74.70% done)\n",
      "Index: 747/1000....(74.80% done)\n",
      "Index: 748/1000....(74.90% done)\n",
      "Index: 749/1000....(75.00% done)\n",
      "Index: 750/1000....(75.10% done)\n",
      "Index: 751/1000....(75.20% done)\n",
      "Index: 752/1000....(75.30% done)\n",
      "Index: 753/1000....(75.40% done)\n",
      "Index: 754/1000....(75.50% done)\n",
      "Index: 755/1000....(75.60% done)\n",
      "Index: 756/1000....(75.70% done)\n",
      "Index: 757/1000....(75.80% done)\n",
      "Index: 758/1000....(75.90% done)\n",
      "Index: 759/1000....(76.00% done)\n",
      "Index: 760/1000....(76.10% done)\n",
      "Index: 761/1000....(76.20% done)\n",
      "Index: 762/1000....(76.30% done)\n",
      "Index: 763/1000....(76.40% done)\n",
      "Index: 764/1000....(76.50% done)\n",
      "Index: 765/1000....(76.60% done)\n",
      "Index: 766/1000....(76.70% done)\n",
      "Index: 767/1000....(76.80% done)\n",
      "Index: 768/1000....(76.90% done)\n",
      "Index: 769/1000....(77.00% done)\n",
      "Index: 770/1000....(77.10% done)\n",
      "Index: 771/1000....(77.20% done)\n",
      "Index: 772/1000....(77.30% done)\n",
      "Index: 773/1000....(77.40% done)\n",
      "Index: 774/1000....(77.50% done)\n",
      "Index: 775/1000....(77.60% done)\n",
      "Index: 776/1000....(77.70% done)\n",
      "Index: 777/1000....(77.80% done)\n",
      "Index: 778/1000....(77.90% done)\n",
      "Index: 779/1000....(78.00% done)\n",
      "Index: 780/1000....(78.10% done)\n",
      "Index: 781/1000....(78.20% done)\n",
      "Index: 782/1000....(78.30% done)\n",
      "Index: 783/1000....(78.40% done)\n",
      "Index: 784/1000....(78.50% done)\n",
      "Index: 785/1000....(78.60% done)\n",
      "Index: 786/1000....(78.70% done)\n",
      "Index: 787/1000....(78.80% done)\n",
      "Index: 788/1000....(78.90% done)\n",
      "Index: 789/1000....(79.00% done)\n",
      "Index: 790/1000....(79.10% done)\n",
      "Index: 791/1000....(79.20% done)\n",
      "Index: 792/1000....(79.30% done)\n",
      "Index: 793/1000....(79.40% done)\n",
      "Index: 794/1000....(79.50% done)\n",
      "Index: 795/1000....(79.60% done)\n",
      "Index: 796/1000....(79.70% done)\n",
      "Index: 797/1000....(79.80% done)\n",
      "Index: 798/1000....(79.90% done)\n",
      "Index: 799/1000....(80.00% done)\n",
      "Index: 800/1000....(80.10% done)\n",
      "Index: 801/1000....(80.20% done)\n",
      "Index: 802/1000....(80.30% done)\n",
      "Index: 803/1000....(80.40% done)\n",
      "Index: 804/1000....(80.50% done)\n",
      "Index: 805/1000....(80.60% done)\n",
      "Index: 806/1000....(80.70% done)\n",
      "Index: 807/1000....(80.80% done)\n",
      "Index: 808/1000....(80.90% done)\n",
      "Index: 809/1000....(81.00% done)\n",
      "Index: 810/1000....(81.10% done)\n",
      "Index: 811/1000....(81.20% done)\n",
      "Index: 812/1000....(81.30% done)\n",
      "Index: 813/1000....(81.40% done)\n",
      "Index: 814/1000....(81.50% done)\n",
      "Index: 815/1000....(81.60% done)\n",
      "Index: 816/1000....(81.70% done)\n",
      "Index: 817/1000....(81.80% done)\n",
      "Index: 818/1000....(81.90% done)\n",
      "Index: 819/1000....(82.00% done)\n",
      "Index: 820/1000....(82.10% done)\n",
      "Index: 821/1000....(82.20% done)\n",
      "Index: 822/1000....(82.30% done)\n",
      "Index: 823/1000....(82.40% done)\n",
      "Index: 824/1000....(82.50% done)\n",
      "Index: 825/1000....(82.60% done)\n",
      "Index: 826/1000....(82.70% done)\n",
      "Index: 827/1000....(82.80% done)\n",
      "Index: 828/1000....(82.90% done)\n",
      "Index: 829/1000....(83.00% done)\n",
      "Index: 830/1000....(83.10% done)\n",
      "Index: 831/1000....(83.20% done)\n",
      "Index: 832/1000....(83.30% done)\n",
      "Index: 833/1000....(83.40% done)\n",
      "Index: 834/1000....(83.50% done)\n",
      "Index: 835/1000....(83.60% done)\n",
      "Index: 836/1000....(83.70% done)\n",
      "Index: 837/1000....(83.80% done)\n",
      "Index: 838/1000....(83.90% done)\n",
      "Index: 839/1000....(84.00% done)\n",
      "Index: 840/1000....(84.10% done)\n",
      "Index: 841/1000....(84.20% done)\n",
      "Index: 842/1000....(84.30% done)\n",
      "Index: 843/1000....(84.40% done)\n",
      "Index: 844/1000....(84.50% done)\n",
      "Index: 845/1000....(84.60% done)\n",
      "Index: 846/1000....(84.70% done)\n",
      "Index: 847/1000....(84.80% done)\n",
      "Index: 848/1000....(84.90% done)\n",
      "Index: 849/1000....(85.00% done)\n",
      "Index: 850/1000....(85.10% done)\n",
      "Index: 851/1000....(85.20% done)\n",
      "Index: 852/1000....(85.30% done)\n",
      "Index: 853/1000....(85.40% done)\n",
      "Index: 854/1000....(85.50% done)\n",
      "Index: 855/1000....(85.60% done)\n",
      "Index: 856/1000....(85.70% done)\n",
      "Index: 857/1000....(85.80% done)\n",
      "Index: 858/1000....(85.90% done)\n",
      "Index: 859/1000....(86.00% done)\n",
      "Index: 860/1000....(86.10% done)\n",
      "Index: 861/1000....(86.20% done)\n",
      "Index: 862/1000....(86.30% done)\n",
      "Index: 863/1000....(86.40% done)\n",
      "Index: 864/1000....(86.50% done)\n",
      "Index: 865/1000....(86.60% done)\n",
      "Index: 866/1000....(86.70% done)\n",
      "Index: 867/1000....(86.80% done)\n",
      "Index: 868/1000....(86.90% done)\n",
      "Index: 869/1000....(87.00% done)\n",
      "Index: 870/1000....(87.10% done)\n",
      "Index: 871/1000....(87.20% done)\n",
      "Index: 872/1000....(87.30% done)\n",
      "Index: 873/1000....(87.40% done)\n",
      "Index: 874/1000....(87.50% done)\n",
      "Index: 875/1000....(87.60% done)\n",
      "Index: 876/1000....(87.70% done)\n",
      "Index: 877/1000....(87.80% done)\n",
      "Index: 878/1000....(87.90% done)\n",
      "Index: 879/1000....(88.00% done)\n",
      "Index: 880/1000....(88.10% done)\n",
      "Index: 881/1000....(88.20% done)\n",
      "Index: 882/1000....(88.30% done)\n",
      "Index: 883/1000....(88.40% done)\n",
      "Index: 884/1000....(88.50% done)\n",
      "Index: 885/1000....(88.60% done)\n",
      "Index: 886/1000....(88.70% done)\n",
      "Index: 887/1000....(88.80% done)\n",
      "Index: 888/1000....(88.90% done)\n",
      "Index: 889/1000....(89.00% done)\n",
      "Index: 890/1000....(89.10% done)\n",
      "Index: 891/1000....(89.20% done)\n",
      "Index: 892/1000....(89.30% done)\n",
      "Index: 893/1000....(89.40% done)\n",
      "Index: 894/1000....(89.50% done)\n",
      "Index: 895/1000....(89.60% done)\n",
      "Index: 896/1000....(89.70% done)\n",
      "Index: 897/1000....(89.80% done)\n",
      "Index: 898/1000....(89.90% done)\n",
      "Index: 899/1000....(90.00% done)\n",
      "Index: 900/1000....(90.10% done)\n",
      "Index: 901/1000....(90.20% done)\n",
      "Index: 902/1000....(90.30% done)\n",
      "Index: 903/1000....(90.40% done)\n",
      "Index: 904/1000....(90.50% done)\n",
      "Index: 905/1000....(90.60% done)\n",
      "Index: 906/1000....(90.70% done)\n",
      "Index: 907/1000....(90.80% done)\n",
      "Index: 908/1000....(90.90% done)\n",
      "Index: 909/1000....(91.00% done)\n",
      "Index: 910/1000....(91.10% done)\n",
      "Index: 911/1000....(91.20% done)\n",
      "Index: 912/1000....(91.30% done)\n",
      "Index: 913/1000....(91.40% done)\n",
      "Index: 914/1000....(91.50% done)\n",
      "Index: 915/1000....(91.60% done)\n",
      "Index: 916/1000....(91.70% done)\n",
      "Index: 917/1000....(91.80% done)\n",
      "Index: 918/1000....(91.90% done)\n",
      "Index: 919/1000....(92.00% done)\n",
      "Index: 920/1000....(92.10% done)\n",
      "Index: 921/1000....(92.20% done)\n",
      "Index: 922/1000....(92.30% done)\n",
      "Index: 923/1000....(92.40% done)\n",
      "Index: 924/1000....(92.50% done)\n",
      "Index: 925/1000....(92.60% done)\n",
      "Index: 926/1000....(92.70% done)\n",
      "Index: 927/1000....(92.80% done)\n",
      "Index: 928/1000....(92.90% done)\n",
      "Index: 929/1000....(93.00% done)\n",
      "Index: 930/1000....(93.10% done)\n",
      "Index: 931/1000....(93.20% done)\n",
      "Index: 932/1000....(93.30% done)\n",
      "Index: 933/1000....(93.40% done)\n",
      "Index: 934/1000....(93.50% done)\n",
      "Index: 935/1000....(93.60% done)\n",
      "Index: 936/1000....(93.70% done)\n",
      "Index: 937/1000....(93.80% done)\n",
      "Index: 938/1000....(93.90% done)\n",
      "Index: 939/1000....(94.00% done)\n",
      "Index: 940/1000....(94.10% done)\n",
      "Index: 941/1000....(94.20% done)\n",
      "Index: 942/1000....(94.30% done)\n",
      "Index: 943/1000....(94.40% done)\n",
      "Index: 944/1000....(94.50% done)\n",
      "Index: 945/1000....(94.60% done)\n",
      "Index: 946/1000....(94.70% done)\n",
      "Index: 947/1000....(94.80% done)\n",
      "Index: 948/1000....(94.90% done)\n",
      "Index: 949/1000....(95.00% done)\n",
      "Index: 950/1000....(95.10% done)\n",
      "Index: 951/1000....(95.20% done)\n",
      "Index: 952/1000....(95.30% done)\n",
      "Index: 953/1000....(95.40% done)\n",
      "Index: 954/1000....(95.50% done)\n",
      "Index: 955/1000....(95.60% done)\n",
      "Index: 956/1000....(95.70% done)\n",
      "Index: 957/1000....(95.80% done)\n",
      "Index: 958/1000....(95.90% done)\n",
      "Index: 959/1000....(96.00% done)\n",
      "Index: 960/1000....(96.10% done)\n",
      "Index: 961/1000....(96.20% done)\n",
      "Index: 962/1000....(96.30% done)\n",
      "Index: 963/1000....(96.40% done)\n",
      "Index: 964/1000....(96.50% done)\n",
      "Index: 965/1000....(96.60% done)\n",
      "Index: 966/1000....(96.70% done)\n",
      "Index: 967/1000....(96.80% done)\n",
      "Index: 968/1000....(96.90% done)\n",
      "Index: 969/1000....(97.00% done)\n",
      "Index: 970/1000....(97.10% done)\n",
      "Index: 971/1000....(97.20% done)\n",
      "Index: 972/1000....(97.30% done)\n",
      "Index: 973/1000....(97.40% done)\n",
      "Index: 974/1000....(97.50% done)\n",
      "Index: 975/1000....(97.60% done)\n",
      "Index: 976/1000....(97.70% done)\n",
      "Index: 977/1000....(97.80% done)\n",
      "Index: 978/1000....(97.90% done)\n",
      "Index: 979/1000....(98.00% done)\n",
      "Index: 980/1000....(98.10% done)\n",
      "Index: 981/1000....(98.20% done)\n",
      "Index: 982/1000....(98.30% done)\n",
      "Index: 983/1000....(98.40% done)\n",
      "Index: 984/1000....(98.50% done)\n",
      "Index: 985/1000....(98.60% done)\n",
      "Index: 986/1000....(98.70% done)\n",
      "Index: 987/1000....(98.80% done)\n",
      "Index: 988/1000....(98.90% done)\n",
      "Index: 989/1000....(99.00% done)\n",
      "Index: 990/1000....(99.10% done)\n",
      "Index: 991/1000....(99.20% done)\n",
      "Index: 992/1000....(99.30% done)\n",
      "Index: 993/1000....(99.40% done)\n",
      "Index: 994/1000....(99.50% done)\n",
      "Index: 995/1000....(99.60% done)\n",
      "Index: 996/1000....(99.70% done)\n",
      "Index: 997/1000....(99.80% done)\n",
      "Index: 998/1000....(99.90% done)\n",
      "Index: 999/1000....(100.00% done)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"skt/kogpt2-base-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "def augment_text(text, num_augmentations=1, max_length=50):\n",
    "    augmented_texts = []\n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "    \n",
    "    for _ in range(num_augmentations):\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=1,\n",
    "            no_repeat_ngram_size=2,\n",
    "            do_sample=True,\n",
    "            top_p=0.8\n",
    "        )\n",
    "        \n",
    "        augmented_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        augmented_texts.append(augmented_text)\n",
    "    \n",
    "    return augmented_texts\n",
    "\n",
    "def augment_dataframe(df, col, num_augmentations=1):\n",
    "    augmented_data = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        print(f\"Index: {_+1}/{len(df)}....({100*(_+1)/len(df):.2f}% done)\")\n",
    "        original_text = row[col]\n",
    "        augmented_texts = augment_text(original_text, num_augmentations)\n",
    "        \n",
    "        for aug_text in augmented_texts:\n",
    "            new_row = row.copy()\n",
    "            new_row[col] = aug_text\n",
    "            augmented_data.append(new_row)\n",
    "    \n",
    "    augmented_df = pd.DataFrame(augmented_data)\n",
    "    return pd.concat([df, augmented_df], ignore_index=True)\n",
    "\n",
    "train = augment_dataframe(train, 'sentence_1')\n",
    "train.tail()\n",
    "\n",
    "train.to_csv('data/train_augmented.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'조여옥을 파면 징계해주세요!!!\\n그리고 박근혜대통령님도 파면조치 해주세요.! 국민들이 청원하는거 다 공감하고 있습니다.\\n이런일이 반복되고 있어 정말 가슴이 아프고 답답합니다.\\n제가 청원합니다.\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.loc[len(train)-1, 'sentence_1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'BATCH_SIZE': 32,\n",
    "    'MAX_LEN': 128,\n",
    "    'LEARNING_RATE': 0.00001,\n",
    "    'EPOCHS': 10,\n",
    "    'MODEL_NAME': 'intfloat/multilingual-e5-small'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:5syk41v7) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">LM_AUG</strong> at: <a href='https://wandb.ai/kangjun205/Level1_STS/runs/5syk41v7' target=\"_blank\">https://wandb.ai/kangjun205/Level1_STS/runs/5syk41v7</a><br/> View project at: <a href='https://wandb.ai/kangjun205/Level1_STS' target=\"_blank\">https://wandb.ai/kangjun205/Level1_STS</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240918_050852-5syk41v7/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:5syk41v7). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/ephemeral/level1-semantictextsimilarity-nlp-16/wandb/run-20240918_051004-m8md1k1y</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kangjun205/Level1_STS/runs/m8md1k1y' target=\"_blank\">LM_AUG</a></strong> to <a href='https://wandb.ai/kangjun205/Level1_STS' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kangjun205/Level1_STS' target=\"_blank\">https://wandb.ai/kangjun205/Level1_STS</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kangjun205/Level1_STS/runs/m8md1k1y' target=\"_blank\">https://wandb.ai/kangjun205/Level1_STS/runs/m8md1k1y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/kangjun205/Level1_STS/runs/m8md1k1y?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f2ad5156410>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key='dfae2bfa701b636a0c1d84ddd928a19f5e17c2f5')\n",
    "wandb.init(project=\"Level1_STS\", name = 'LM_AUG', config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(val_check_interval=1)` was configured so validation will run after every batch.\n",
      "/opt/conda/envs/STS/lib/python3.11/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/opt/conda/envs/STS/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /data/ephemeral/level1-semantictextsimilarity-nlp-16/saved exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type      | Params | Mode \n",
      "----------------------------------------------\n",
      "0 | mod     | BertModel | 117 M  | eval \n",
      "1 | dense   | Linear    | 385    | train\n",
      "2 | sigmoid | Sigmoid   | 0      | train\n",
      "----------------------------------------------\n",
      "117 M     Trainable params\n",
      "0         Non-trainable params\n",
      "117 M     Total params\n",
      "470.617   Total estimated model params size (MB)\n",
      "2         Modules in train mode\n",
      "228       Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/STS/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/STS/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 2/583 [00:00<03:50,  2.52it/s, v_num=1k1y]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/STS/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: The variance of predictions or target is close to zero. This can cause instability in Pearson correlationcoefficient, leading to wrong results. Consider re-scaling the input if possible or computing using alarger dtype (currently using torch.float32).\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 583/583 [06:52<00:00,  1.41it/s, v_num=1k1y]\n"
     ]
    }
   ],
   "source": [
    "now_min = datetime.datetime.now().strftime('%d%H%M')\n",
    "now_sec = datetime.datetime.now().strftime('%d%H%M%S')\n",
    "\n",
    "tokenizer = get_tokenizer(config['MODEL_NAME'])\n",
    "dataloader = TextDataLoader(\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=config['MAX_LEN'],\n",
    "    train_data=train,\n",
    "    dev_data=dev,\n",
    "    truncation=True,\n",
    "    batch_size=config['BATCH_SIZE']\n",
    ")\n",
    "model = STSModel(config)\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath='saved',\n",
    "    filename=f'best-model-{now_sec}',\n",
    "    save_top_k=3,\n",
    "    monitor='val_loss',\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "wandb_checkpoint_callback = WandbCheckpointCallback(top_k=3)\n",
    "\n",
    "model_name = config['MODEL_NAME']\n",
    "run_name = f'{model_name}-{now_min}'\n",
    "wandb_logger = WandbLogger(name = run_name, project=\"Level1-STS\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    max_epochs=config['EPOCHS'],\n",
    "    val_check_interval=1,\n",
    "    callbacks=[early_stop_callback, checkpoint_callback],\n",
    "    logger = wandb_logger\n",
    "    )\n",
    "\n",
    "trainer.fit(model, datamodule=dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/envs/STS/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/STS/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'TestDataset' object has no attribute 'labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 15\u001b[0m\n\u001b[1;32m      2\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m TextDataLoader(\n\u001b[1;32m      3\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[1;32m      4\u001b[0m     max_len\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMAX_LEN\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBATCH_SIZE\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m     10\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     11\u001b[0m     accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m     devices\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     13\u001b[0m )\n\u001b[0;32m---> 15\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/STS/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:858\u001b[0m, in \u001b[0;36mTrainer.predict\u001b[0;34m(self, model, dataloaders, datamodule, return_predictions, ckpt_path)\u001b[0m\n\u001b[1;32m    856\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    857\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredicting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 858\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_predictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/STS/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     50\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m/opt/conda/envs/STS/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:897\u001b[0m, in \u001b[0;36mTrainer._predict_impl\u001b[0;34m(self, model, dataloaders, datamodule, return_predictions, ckpt_path)\u001b[0m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    894\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    895\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn, ckpt_path, model_provided\u001b[38;5;241m=\u001b[39mmodel_provided, model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    896\u001b[0m )\n\u001b[0;32m--> 897\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredicting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/STS/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:981\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    978\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 981\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    986\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/STS/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1020\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1018\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evaluation_loop\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m   1019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredicting:\n\u001b[0;32m-> 1020\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n",
      "File \u001b[0;32m/opt/conda/envs/STS/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:178\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/STS/lib/python3.11/site-packages/pytorch_lightning/loops/prediction_loop.py:121\u001b[0m, in \u001b[0;36m_PredictionLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m     dataloader_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 121\u001b[0m     batch, batch_idx, dataloader_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mis_last_batch \u001b[38;5;241m=\u001b[39m data_fetcher\u001b[38;5;241m.\u001b[39mdone\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/STS/lib/python3.11/site-packages/pytorch_lightning/loops/fetchers.py:133\u001b[0m, in \u001b[0;36m_PrefetchDataFetcher.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatches\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;66;03m# this will run only when no pre-fetching was done.\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__next__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;66;03m# the iterator is empty\u001b[39;00m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/STS/lib/python3.11/site-packages/pytorch_lightning/loops/fetchers.py:60\u001b[0m, in \u001b[0;36m_DataFetcher.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_profiler()\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 60\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/STS/lib/python3.11/site-packages/pytorch_lightning/utilities/combined_loader.py:341\u001b[0m, in \u001b[0;36mCombinedLoader.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _ITERATOR_RETURN:\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 341\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator)\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator, _Sequential):\n\u001b[1;32m    343\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/opt/conda/envs/STS/lib/python3.11/site-packages/pytorch_lightning/utilities/combined_loader.py:142\u001b[0m, in \u001b[0;36m_Sequential.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 142\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterators[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;66;03m# try the next iterator\u001b[39;00m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_next_iterator()\n",
      "File \u001b[0;32m/opt/conda/envs/STS/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/conda/envs/STS/lib/python3.11/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/conda/envs/STS/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/opt/conda/envs/STS/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/data/ephemeral/level1-semantictextsimilarity-nlp-16/data_loader/datasets.py:54\u001b[0m, in \u001b[0;36mTestDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     52\u001b[0m sentence_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentence_1[idx]\n\u001b[1;32m     53\u001b[0m sentence_2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentence_2[idx]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabels\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     55\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[idx]\n\u001b[1;32m     57\u001b[0m encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(\n\u001b[1;32m     58\u001b[0m     sentence_1,\n\u001b[1;32m     59\u001b[0m     sentence_2,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     67\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TestDataset' object has no attribute 'labels'"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv('data/test.csv')\n",
    "dataloader = TextDataLoader(\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=config['MAX_LEN'],\n",
    "    test_data=test,\n",
    "    truncation=True,\n",
    "    batch_size=config['BATCH_SIZE']\n",
    ")\n",
    "    \n",
    "trainer = Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1\n",
    ")\n",
    "\n",
    "trainer.predict(model, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
