{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 유의어 기반 증강"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import wordnet\n",
    "import random\n",
    "\n",
    "# CSV 파일 로드\n",
    "file_path = 'train.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 유의어 대체 함수\n",
    "def synonym_replacement(sentence, n=1):\n",
    "    words = sentence.split()\n",
    "    new_words = words[:]\n",
    "    random_word_list = list(set([word for word in words if wordnet.synsets(word)]))\n",
    "    \n",
    "    # random_word_list가 비어있지 않은지 확인\n",
    "    if len(random_word_list) == 0:\n",
    "        return sentence  # 동의어가 없을 경우 원래 문장 반환\n",
    "    \n",
    "    for _ in range(n):\n",
    "        word_to_replace = random.choice(random_word_list)\n",
    "        synonyms = wordnet.synsets(word_to_replace)\n",
    "        synonym = random.choice(synonyms).lemmas()[0].name()\n",
    "        new_words = [synonym if word == word_to_replace else word for word in new_words]\n",
    "        \n",
    "    return \" \".join(new_words)\n",
    "\n",
    "# 증강 데이터 생성\n",
    "augmented_data = []\n",
    "last_id_number = len(df)\n",
    "\n",
    "for i in range(len(df)):\n",
    "    new_sentence_1 = synonym_replacement(df.loc[i, 'sentence_1'])\n",
    "    new_sentence_2 = synonym_replacement(df.loc[i, 'sentence_2'])\n",
    "    \n",
    "    augmented_data.append({\n",
    "        'id': f\"augment-{last_id_number + i}\",\n",
    "        'source': 'augment',\n",
    "        'sentence_1': new_sentence_1,\n",
    "        'sentence_2': new_sentence_2,\n",
    "        'label': df.loc[i, 'label'],\n",
    "        'binary-label': df.loc[i, 'binary-label']\n",
    "    })\n",
    "\n",
    "# 증강 데이터를 데이터프레임으로 변환\n",
    "df_augmented = pd.DataFrame(augmented_data)\n",
    "\n",
    "# 기존 데이터와 증강 데이터 합치기\n",
    "combined_df = pd.concat([df, df_augmented], ignore_index=True)\n",
    "\n",
    "# 합쳐진 데이터를 새로운 CSV 파일로 저장\n",
    "combined_file_path = 'combined_train_augmented.csv'\n",
    "combined_df.to_csv(combined_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 역번영 기반 데이터 증강"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# MarianMT 모델과 토크나이저 로드\n",
    "def load_model(src_lang, tgt_lang):\n",
    "    model_name = f'Helsinki-NLP/opus-mt-tc-big-{src_lang}-{tgt_lang}'\n",
    "    tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "    model = MarianMTModel.from_pretrained(model_name)\n",
    "    return tokenizer, model\n",
    "\n",
    "# 번역 함수\n",
    "def translate_sentence(sentence, tokenizer, model):\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True)\n",
    "    translated = model.generate(**inputs)\n",
    "    return tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "\n",
    "# 역번역 함수 (한글 -> 영어 -> 한글)\n",
    "def back_translation(sentence):\n",
    "    # 한글 -> 영어 모델 로드\n",
    "    tokenizer_ko_en, model_ko_en = load_model('ko', 'en')\n",
    "    \n",
    "    # 영어 -> 한글 모델 로드\n",
    "    tokenizer_en_ko, model_en_ko = load_model('en', 'ko')\n",
    "    \n",
    "    # 한글 -> 영어 번역\n",
    "    translated_to_en = translate_sentence(sentence, tokenizer_ko_en, model_ko_en)\n",
    "    \n",
    "    # 영어 -> 한글 번역\n",
    "    back_translated_to_ko = translate_sentence(translated_to_en, tokenizer_en_ko, model_en_ko)\n",
    "    \n",
    "    return back_translated_to_ko\n",
    "\n",
    "# CSV 파일 로드\n",
    "file_path = 'train.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 증강 데이터 생성\n",
    "augmented_data = []\n",
    "last_id_number = len(df)\n",
    "\n",
    "for i in range(len(df)):\n",
    "    new_sentence_1 = back_translation(df.loc[i, 'sentence_1'])\n",
    "    new_sentence_2 = back_translation(df.loc[i, 'sentence_2'])\n",
    "    \n",
    "    augmented_data.append({\n",
    "        'id': f\"augment-{last_id_number + i}\",\n",
    "        'source': 'augment',\n",
    "        'sentence_1': new_sentence_1,\n",
    "        'sentence_2': new_sentence_2,\n",
    "        'label': df.loc[i, 'label'],\n",
    "        'binary-label': df.loc[i, 'binary-label']\n",
    "    })\n",
    "\n",
    "# 증강 데이터를 데이터프레임으로 변환\n",
    "df_augmented = pd.DataFrame(augmented_data)\n",
    "\n",
    "# 기존 데이터와 증강 데이터 합치기\n",
    "combined_df = pd.concat([df, df_augmented], ignore_index=True)\n",
    "\n",
    "# 합쳐진 데이터를 새로운 CSV 파일로 저장\n",
    "combined_file_path = 'combined_train_augmented_back_translation.csv'\n",
    "combined_df.to_csv(combined_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. BERT기반 단어 대체 증강"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "import random\n",
    "\n",
    "# CSV 파일 로드\n",
    "file_path = 'train.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# BERT 기반 단어 대체 함수\n",
    "def bert_replacement(sentence):\n",
    "    fill_mask = pipeline('fill-mask', model='bert-base-uncased')\n",
    "    words = sentence.split()\n",
    "    if len(words) < 4:\n",
    "        return sentence\n",
    "    mask_idx = random.randint(0, len(words)-1)\n",
    "    words[mask_idx] = '[MASK]'\n",
    "    new_sentence = \" \".join(words)\n",
    "    result = fill_mask(new_sentence)[0]['sequence'].replace('[CLS] ', '').replace(' [SEP]', '')\n",
    "    return result\n",
    "\n",
    "# 증강 데이터 생성\n",
    "augmented_data = []\n",
    "last_id_number = len(df)\n",
    "\n",
    "for i in range(len(df)):\n",
    "    new_sentence_1 = bert_replacement(df.loc[i, 'sentence_1'])\n",
    "    new_sentence_2 = bert_replacement(df.loc[i, 'sentence_2'])\n",
    "    \n",
    "    augmented_data.append({\n",
    "        'id': f\"augment-{last_id_number + i}\",\n",
    "        'source': 'augment',\n",
    "        'sentence_1': new_sentence_1,\n",
    "        'sentence_2': new_sentence_2,\n",
    "        'label': df.loc[i, 'label'],\n",
    "        'binary-label': df.loc[i, 'binary-label']\n",
    "    })\n",
    "\n",
    "# 증강 데이터를 데이터프레임으로 변환\n",
    "df_augmented = pd.DataFrame(augmented_data)\n",
    "\n",
    "# 기존 데이터와 증강 데이터 합치기\n",
    "combined_df = pd.concat([df, df_augmented], ignore_index=True)\n",
    "\n",
    "# 합쳐진 데이터를 새로운 CSV 파일로 저장\n",
    "combined_file_path = 'combined_train_augmented_bert_replacement.csv'\n",
    "combined_df.to_csv(combined_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Pegasus 기반 패러프레이징"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
    "\n",
    "# CSV 파일 로드\n",
    "file_path = 'train.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Pegasus 기반 패러프레이징 함수\n",
    "def paraphrasing(sentence):\n",
    "    model_name = \"tuner007/pegasus_paraphrase\"\n",
    "    tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "    model = PegasusForConditionalGeneration.from_pretrained(model_name)\n",
    "    \n",
    "    batch = tokenizer.prepare_seq2seq_batch([sentence], truncation=True, padding='longest', return_tensors=\"pt\")\n",
    "    translated = model.generate(**batch)\n",
    "    paraphrased_sentence = tokenizer.batch_decode(translated, skip_special_tokens=True)[0]\n",
    "    \n",
    "    return paraphrased_sentence\n",
    "\n",
    "# 증강 데이터 생성\n",
    "augmented_data = []\n",
    "last_id_number = len(df)\n",
    "\n",
    "for i in range(len(df)):\n",
    "    new_sentence_1 = paraphrasing(df.loc[i, 'sentence_1'])\n",
    "    new_sentence_2 = paraphrasing(df.loc[i, 'sentence_2'])\n",
    "    \n",
    "    augmented_data.append({\n",
    "        'id': f\"augment-{last_id_number + i}\",\n",
    "        'source': 'augment',\n",
    "        'sentence_1': new_sentence_1,\n",
    "        'sentence_2': new_sentence_2,\n",
    "        'label': df.loc[i, 'label'],\n",
    "        'binary-label': df.loc[i, 'binary-label']\n",
    "    })\n",
    "\n",
    "# 증강 데이터를 데이터프레임으로 변환\n",
    "df_augmented = pd.DataFrame(augmented_data)\n",
    "\n",
    "# 기존 데이터와 증강 데이터 합치기\n",
    "combined_df = pd.concat([df, df_augmented], ignore_index=True)\n",
    "\n",
    "# 합쳐진 데이터를 새로운 CSV 파일로 저장\n",
    "combined_file_path = 'combined_train_augmented_paraphrasing.csv'\n",
    "combined_df.to_csv(combined_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. EDA 기반 랜덤 단어 삭제 증강"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# CSV 파일 로드\n",
    "file_path = 'train.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# EDA - 단어 랜덤 삭제 함수\n",
    "def random_deletion(sentence, p=0.3):\n",
    "    words = sentence.split()\n",
    "    if len(words) == 1:\n",
    "        return sentence\n",
    "    remaining_words = [word for word in words if random.uniform(0, 1) > p]\n",
    "    if len(remaining_words) == 0:\n",
    "        remaining_words = [random.choice(words)]\n",
    "    return \" \".join(remaining_words)\n",
    "\n",
    "# 증강 데이터 생성\n",
    "augmented_data = []\n",
    "last_id_number = len(df)\n",
    "\n",
    "for i in range(len(df)):\n",
    "    new_sentence_1 = random_deletion(df.loc[i, 'sentence_1'])\n",
    "    new_sentence_2 = random_deletion(df.loc[i, 'sentence_2'])\n",
    "    \n",
    "    augmented_data.append({\n",
    "        'id': f\"augment-{last_id_number + i}\",\n",
    "        'source': 'augment',\n",
    "        'sentence_1': new_sentence_1,\n",
    "        'sentence_2': new_sentence_2,\n",
    "        'label': df.loc[i, 'label'],\n",
    "        'binary-label': df.loc[i, 'binary-label']\n",
    "    })\n",
    "\n",
    "# 증강 데이터를 데이터프레임으로 변환\n",
    "df_augmented = pd.DataFrame(augmented_data)\n",
    "\n",
    "# 기존 데이터와 증강 데이터 합치기\n",
    "combined_df = pd.concat([df, df_augmented], ignore_index=True)\n",
    "\n",
    "# 합쳐진 데이터를 새로운 CSV 파일로 저장\n",
    "combined_file_path = 'combined_train_augmented_eda.csv'\n",
    "combined_df.to_csv(combined_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.  TF-IDF 기반 단어 삭제 증강"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# CSV 파일 로드\n",
    "file_path = 'train.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# TF-IDF 기반 단어 삭제 함수\n",
    "def tfidf_based_deletion(sentences, threshold=0.05):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.fit(sentences)\n",
    "    \n",
    "    new_sentences = []\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        tfidf_scores = vectorizer.transform([sentence]).toarray()[0]\n",
    "        important_words = [words[i] for i in range(len(words)) if tfidf_scores[i] > threshold]\n",
    "        if len(important_words) < 3:\n",
    "            important_words = words[:3]  # 최소한 3개의 단어는 유지\n",
    "        new_sentence = \" \".join(important_words)\n",
    "        new_sentences.append(new_sentence)\n",
    "    \n",
    "    return new_sentences\n",
    "\n",
    "# 증강 데이터 생성\n",
    "augmented_data = []\n",
    "last_id_number = len(df)\n",
    "sentences_1 = df['sentence_1'].tolist()\n",
    "sentences_2 = df['sentence_2'].tolist()\n",
    "\n",
    "new_sentences_1 = tfidf_based_deletion(sentences_1)\n",
    "new_sentences_2 = tfidf_based_deletion(sentences_2)\n",
    "\n",
    "for i in range(len(df)):\n",
    "    augmented_data.append({\n",
    "        'id': f\"augment-{last_id_number + i}\",\n",
    "        'source': 'augment',\n",
    "        'sentence_1': new_sentences_1[i],\n",
    "        'sentence_2': new_sentences_2[i],\n",
    "        'label': df.loc[i, 'label'],\n",
    "        'binary-label': df.loc[i, 'binary-label']\n",
    "    })\n",
    "\n",
    "# 증강 데이터를 데이터프레임으로 변환\n",
    "df_augmented = pd.DataFrame(augmented_data)\n",
    "\n",
    "# 기존 데이터와 증강 데이터 합치기\n",
    "combined_df = pd.concat([df, df_augmented], ignore_index=True)\n",
    "\n",
    "# 합쳐진 데이터를 새로운 CSV 파일로 저장\n",
    "combined_file_path = 'combined_train_augmented_tfidf_deletion.csv'\n",
    "combined_df.to_csv(combined_file_path, index=False)i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. 문장 순서 변경 데이터 증강"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# CSV 파일 로드\n",
    "file_path = 'train.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 문장 1과 문장 2의 순서를 바꿔주는 함수\n",
    "def switch_sentences(sentence_1, sentence_2):\n",
    "    return sentence_2, sentence_1\n",
    "\n",
    "# 증강 데이터 생성\n",
    "augmented_data = []\n",
    "last_id_number = len(df)\n",
    "\n",
    "for i in range(len(df)):\n",
    "    # 문장 1과 문장 2의 순서를 바꿔서 새로운 데이터 생성\n",
    "    new_sentence_1, new_sentence_2 = switch_sentences(df.loc[i, 'sentence_1'], df.loc[i, 'sentence_2'])\n",
    "    \n",
    "    # 증강된 데이터를 리스트에 추가\n",
    "    augmented_data.append({\n",
    "        'id': f\"augment-{last_id_number + i}\",\n",
    "        'source': 'augment',\n",
    "        'sentence_1': new_sentence_1,\n",
    "        'sentence_2': new_sentence_2,\n",
    "        'label': df.loc[i, 'label'],\n",
    "        'binary-label': df.loc[i, 'binary-label']\n",
    "    })\n",
    "\n",
    "# 증강 데이터를 데이터프레임으로 변환\n",
    "df_augmented = pd.DataFrame(augmented_data)\n",
    "\n",
    "# 기존 데이터와 증강 데이터 합치기\n",
    "combined_df = pd.concat([df, df_augmented], ignore_index=True)\n",
    "\n",
    "# 합쳐진 데이터를 새로운 CSV 파일로 저장\n",
    "combined_file_path = 'combined_train_augmented_switched.csv'\n",
    "combined_df.to_csv(combined_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 데이터 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#삭제할 데이터 인덱스\n",
    "\n",
    "ids_to_drop = [131, 132, 134, 146, 154, 155,\n",
    "                186, 187, 202, 219, 253, 277,\n",
    "                294, 311, 348, 360 ,380 ,385, \n",
    "                447, 455, 474, 481, 488, 493, \n",
    "                558, 559, 563, 564, 643, 679, \n",
    "                692, 772, 810, 881, 935, 951,\n",
    "                7231, 7378, 7396, 7652, 7665,\n",
    "                7690, 7898]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 'id' 열에서 끝부분의 숫자를 추출하여 'id_numeric'이라는 새로운 열에 저장\n",
    "df['id_numeric'] = df['id'].apply(lambda x: int(x.split('-')[-1]))\n",
    "\n",
    "# 2. 'id_numeric' 값이 우리가 삭제하려는 ID 목록(ids_to_drop)에 포함되지 않은 행들만 남겨서 새로운 데이터프레임을 생성\n",
    "df_cleaned = df[~df['id_numeric'].isin(ids_to_drop)]\n",
    "\n",
    "# 3. 더 이상 필요 없는 'id_numeric' 열을 삭제\n",
    "df_cleaned = df_cleaned.drop(columns=['id_numeric'])\n",
    "\n",
    "# 4. 수정된 데이터프레임을 CSV 파일로 저장\n",
    "cleaned_file_path_v3 = 'cleaned_augmented_train_v3.csv'\n",
    "df_cleaned.to_csv(cleaned_file_path_v3, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
